{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow \n",
    "# !pip install numpy \n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install scikit-learn\n",
    "# !pip install seaborn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU usage for tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi # veriying if NVIDEA drive and CUDA runtime loads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "\n",
    "# 1) Ver todas as GPUs\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(\"GPUs detectadas:\", gpus)\n",
    "\n",
    "if gpus:\n",
    "    # 2) (Opcional) limitar a visão só à primeira GPU\n",
    "    tf.config.set_visible_devices(gpus[0], \"GPU\")\n",
    "\n",
    "    # 3) (Recomendado) liberar memória sob demanda\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBS\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import keras\n",
    "\n",
    "register_matplotlib_converters()\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "plt.rc(\"figure\", figsize=(16, 6))\n",
    "plt.rc(\"font\", size=13)\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize = (16, 6), dpi = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gridsearch\n",
    "def grid_search_cv(modelo, units, X_train, learning_rates, y_train, epochs_list, batch_sizes, patiences, model_name):\n",
    "    best_loss = float('inf')\n",
    "    best_params = {}\n",
    "    for lr in learning_rates:\n",
    "        for epochs in epochs_list:\n",
    "            for batch_size in batch_sizes:\n",
    "                for patience in patiences:\n",
    "                    model = modelo(units, X_train, lr)\n",
    "                    histories = fit_model_with_cross_validation(model, X_train, y_train, model_name, patience, epochs, batch_size)\n",
    "                    mean_history = calculate_mean_history(histories)\n",
    "                    val_loss = min(mean_history['val_loss'])\n",
    "                    print(\"Val Loss: \", val_loss, \"learning rate: \", lr, \"epochs: \",  epochs, \"batch_size: \" , batch_size, \"patience: \", patience)\n",
    "                    if val_loss < best_loss:\n",
    "                        best_loss = val_loss\n",
    "                        best_params = {'learning_rate': lr, 'epochs': epochs, 'batch_size': batch_size, 'patience': patience} \n",
    "    print('O modelo '+model_name+ ' tem como melhores parametros os seguintes: learning_rate '+ str(best_params['learning_rate'])+' epochs: '+ str(best_params['epochs'])+' batch_size: '+ str(best_params['batch_size'])+ ' patience: '+ str(best_params['patience']))\n",
    "    return best_params\n",
    "\n",
    "#validação cruzada\n",
    "def fit_model_with_cross_validation(model, xtrain, ytrain, model_name, patience, epochs, batch_size):\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    fold = 1\n",
    "    histories = []\n",
    "    for train_index, val_index in tscv.split(xtrain):\n",
    "        x_train_fold, x_val_fold = xtrain[train_index], xtrain[val_index]\n",
    "        y_train_fold, y_val_fold = ytrain[train_index], ytrain[val_index]\n",
    "        early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True, min_delta=1e-5)\n",
    "        history = model.fit(x_train_fold, y_train_fold, epochs=epochs, validation_data=(x_val_fold, y_val_fold), batch_size=batch_size, callbacks=[early_stop], verbose=1)\n",
    "        print('\\n\\nTREINAMENTO - Fold', fold, 'do modelo:', model_name)\n",
    "        histories.append(history)\n",
    "        fold += 1   \n",
    "    return histories \n",
    "\n",
    "# calcula a media das metricas obtidas nos historys - validação cruzada\n",
    "def calculate_mean_history(histories):\n",
    "    mean_history = {'loss': [], 'root_mean_squared_error': [], 'val_loss': [], 'val_root_mean_squared_error': []}\n",
    "    for fold_history in histories:\n",
    "        for key in mean_history.keys():\n",
    "            mean_history[key].append(fold_history.history[key])\n",
    "    for key, values in mean_history.items():\n",
    "        max_len = max(len(val) for val in values)\n",
    "        for i in range(len(values)):\n",
    "            if len(values[i]) < max_len: #caso em que nao se treina todas as epocas (patience)\n",
    "                values[i] += [values[i][-1]] * (max_len - len(values[i])) #completa o restante da lista com o ultimo valor obtido\n",
    "    for key, values in mean_history.items():\n",
    "        mean_history[key] = [sum(vals) / len(vals) for vals in zip(*values)]\n",
    "    \n",
    "    return mean_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input dataset\n",
    "# The input shape should be [samples, time steps, features\n",
    "def create_dataset (X, look_back = 3):\n",
    "    Xs, ys = [], []\n",
    "    \n",
    "    for i in range(len(X)-look_back):\n",
    "        v = X[i:i+look_back]\n",
    "        Xs.append(v)\n",
    "        ys.append(X[i+look_back])\n",
    "        \n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Create LSTM model\n",
    "def create_lstm(units, train, learning_rate): \n",
    "    model = Sequential() \n",
    "    # Old Config\n",
    "    model.add(LSTM(units = units, return_sequences = True, input_shape = [train.shape[1], train.shape[2]]))\n",
    "    model.add(LSTM(units = units)) \n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    # model.compile(loss=MeanSquaredError(), optimizer = Adam(learning_rate=learning_rate), metrics=[RootMeanSquaredError()])\n",
    "    model.compile(\n",
    "        loss=Huber(delta=0.25),  # delta define quando a perda muda de quadrática para linear\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        metrics=[RootMeanSquaredError()]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "#treinamento do modelo\n",
    "def fit_model(model, xtrain, ytrain, model_name, patience, epochs, batch_size ):\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = patience, restore_best_weights=True)\n",
    "    history = model.fit(xtrain, ytrain, epochs = epochs, validation_split = 0.2, batch_size = batch_size, shuffle = True, callbacks=[early_stop]) \n",
    "    print('\\n\\nTREINAMENTO: ' + model_name)\n",
    "    return history\n",
    "\n",
    "# Make prediction\n",
    "def prediction(model, xtest, ytest, myscaler, model_name, link): \n",
    "    prediction = model.predict(xtest) \n",
    "    prediction = myscaler.inverse_transform(prediction) \n",
    "    # dataframe_prediction = pd.DataFrame(data={'Predições':prediction.flatten()})\n",
    "    dataframe_prediction = pd.DataFrame(data={'Prediction':prediction.flatten(), 'Test':ytest.flatten()})\n",
    "    #save_path = os.path.join('..', '..', 'predicoes', f'prediction {model_name} {link}.csv') \n",
    "    save_path = os.path.join('..', '..', 'results', 'bi-lstm', 'forecast', f'prediction {model_name} {link}.csv') \n",
    "    dataframe_prediction.to_csv(save_path)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "# Calculate MAE and RMSE\n",
    "def evaluate_prediction(predictions, actual, model_name):\n",
    "    errors = predictions - actual\n",
    "    mse = np.square(errors).mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "    nrmse = rmse/ np.max(actual)\n",
    "    mae = np.abs(errors).mean()\n",
    "    print(model_name + ':')\n",
    "    print('Mean Absolute Error: {:.4f}'.format(mae))\n",
    "    print('Root Mean Square Error: {:.4f}'.format(rmse))\n",
    "    print('Normalized Root Mean Square Error: {:.4f}%'.format(nrmse*100))\n",
    "    print('')\n",
    "\n",
    "    return rmse, mae, nrmse, model_name\n",
    "\n",
    "# ===================================================================================\n",
    "# NOVA FUNÇÃO: valida métrica apenas em pontos com dado real (mask == True)\n",
    "# ===================================================================================\n",
    "def validate_missing_data_prediction(predictions, actual, mask, model_name):\n",
    "    \"\"\"\n",
    "    Calcula RMSE, MAE e NRMSE APENAS nos pontos onde há dado real (mask==True).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions : np.ndarray  (shape: [N, 1] ou [N])\n",
    "        Vetor de predições PURO, sem mistura com pontos reais.\n",
    "    actual      : np.ndarray  (shape: [N, 1] ou [N])\n",
    "        Vetor de valores reais correspondentes.\n",
    "    mask        : np.ndarray  (shape: [N], dtype=bool)\n",
    "        True  -> ponto com dado real\n",
    "        False -> ponto ausente (gap) onde não devemos avaliar.\n",
    "    model_name  : str\n",
    "        Identificador do modelo (ex.: 'LSTM')\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (rmse, mae, nrmse, model_name)\n",
    "    \"\"\"\n",
    "    # Garante formato 1-D\n",
    "    predictions = predictions.flatten()\n",
    "    actual      = actual.flatten()\n",
    "    mask        = mask.astype(bool).flatten()\n",
    "\n",
    "    # Seleciona apenas os pontos válidos\n",
    "    preds_valid = predictions[mask]\n",
    "    acts_valid  = actual[mask]\n",
    "\n",
    "    if len(acts_valid) == 0:\n",
    "        print(f\"{model_name}: NÃO HÁ PONTOS REAIS PARA AVALIAÇÃO NESTA JANELA!\")\n",
    "        return np.nan, np.nan, np.nan, model_name\n",
    "\n",
    "    errors = preds_valid - acts_valid\n",
    "    mse    = np.square(errors).mean()\n",
    "    rmse   = np.sqrt(mse)\n",
    "    nrmse  = rmse / ((acts_valid.max() - acts_valid.min()) + 1e-12)\n",
    "    mae    = np.abs(errors).mean()\n",
    "\n",
    "    print(model_name + ' (missing-aware):')\n",
    "    print(f'MAE:  {mae:.4f}')\n",
    "    print(f'RMSE: {rmse:.4f}')\n",
    "    print(f'NRMSE:{nrmse*100:.4f}%\\n')\n",
    "\n",
    "    return rmse, mae, nrmse, model_name\n",
    "\n",
    "def evaluate_prediction(predictions, actual, model_name):\n",
    "    errors = predictions - actual\n",
    "    mse = np.square(errors).mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "    nrmse = rmse/ ((np.max(actual))-(np.min(actual)))\n",
    "    mae = np.abs(errors).mean()\n",
    "    print(model_name + ':')\n",
    "    print('Mean Absolute Error: {:.4f}'.format(mae))\n",
    "    print('Root Mean Square Error: {:.4f}'.format(rmse))\n",
    "    print('Normalized Root Mean Square Error: {:.4f}%'.format(nrmse*100))\n",
    "    print('')\n",
    "\n",
    "    return rmse, mae, nrmse, model_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bits_para_megabits(df, col_vaz):\n",
    "    df[col_vaz] = df[col_vaz]/1000000\n",
    "    df[col_vaz] = df[col_vaz].replace(-1, df[col_vaz].mean())\n",
    "    df[col_vaz] = df[col_vaz].fillna(df[col_vaz].mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "def linear_interpolation(df, limit_direction='both', method='linear'):\n",
    "    df_imputed = df.interpolate(method=method, limit_direction=limit_direction)\n",
    "\n",
    "    df['Throughput'] = df['Throughput'].fillna(df_imputed['Throughput'])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                \n",
    "def visualizacao_series(df, col_vazao, titulo):\n",
    "    df[col_vazao].plot(figsize=(18,6))\n",
    "    plt.title(titulo)\n",
    "    plt.ylabel('Vazao (Mbits/s)')\n",
    "    plt.legend() \n",
    "    plt.show()\n",
    "\n",
    "#plotar os graficos da media dos treinamentos por epocas: validação cruzada\n",
    "def plot_loss_cv(mean_history, model_name, link):\n",
    "    epochs = range(1, len(mean_history['loss']) + 1)\n",
    "    plt.plot(epochs, mean_history['loss'], label='Train Loss')\n",
    "    plt.plot(epochs, mean_history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Mean Training and Validation Loss for '+' '+link + ' '+ model_name)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_rmse_cv(mean_history):\n",
    "    epochs = range(1, len(mean_history['root_mean_squared_error']) + 1)\n",
    "    plt.plot(epochs, mean_history['root_mean_squared_error'], label='Train RMSE')\n",
    "    plt.plot(epochs, mean_history['val_root_mean_squared_error'], label='Validation RMSE')\n",
    "    plt.title('Mean Training and Validation RMSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "########################################### plote dos graficos de treinamento ###################################################################################\n",
    " #Plot train loss and validation loss\n",
    "def plot_loss(history, model_name, link):\n",
    "     plt.figure(figsize = (15, 6), dpi=100)\n",
    "     plt.plot(history.history['loss'])\n",
    "     plt.plot(history.history['val_loss'])\n",
    "     plt.title('Model Train vs Validation Loss for '+' '+link + ' '+ model_name)\n",
    "     plt.ylabel('Loss')\n",
    "     plt.xlabel('Epoch')\n",
    "     plt.legend(['Train loss', 'Validation loss'], loc='upper right')\n",
    "def plot_rmse(history, model_name, link):\n",
    "     plt.figure(figsize = (15, 6), dpi=100)\n",
    "     plt.plot(history.history['rmse'])\n",
    "     plt.plot(history.history['val_rmse'])\n",
    "     plt.title('Model Train vs RMSE for '+' '+link + ' '+ model_name)\n",
    "     plt.ylabel('rmse')\n",
    "     plt.xlabel('Epoch')\n",
    "     plt.legend(['Train rmse', 'Validation loss'], loc='upper right')\n",
    "################################################################################################################################################################\n",
    " \n",
    "\n",
    "def plot_future(predictionLSTM, y_test, link):\n",
    "    plt.figure(figsize=(15, 6), dpi=100)\n",
    "    range_future = len(y_test)\n",
    "    plt.plot(np.arange(range_future), np.array(y_test), label='Test data')\n",
    "    plt.plot(np.arange(range_future), np.array(predictionLSTM), label='LSTM')\n",
    "    # dict_to_dataframe_prediction = {\n",
    "    #     # \"range_future\": np.arange(range_future),\n",
    "    #     f\"prediction{model_name}\": np.array(prediction.squeeze())\n",
    "    # }\n",
    "    \n",
    "    plt.title('Test data vs prediction for '+ link)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Mbis/s')\n",
    "    save_path = os.path.join('..', '..', 'results', 'bi-lstm', 'plots', link + '.png')\n",
    "    save_path = os.path.normpath(save_path)  \n",
    "\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    try:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"A figura foi salva com sucesso em: {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar a figura: {e}\")\n",
    "    plt.show()\n",
    "\n",
    "    # #Tenta salvar a fig\n",
    "    # save_path = os.path.join('..', '..', 'graficos', 'predicoes', 'round_2', 'graficos', link + '.png')\n",
    "\n",
    "    # #save_path = '../../graficos/predicoes/round_2/graficos/' + link + '.png'\n",
    "    # try:\n",
    "    #     plt.savefig(save_path)\n",
    "    #     print(f\"A figura foi salva com sucesso em: {save_path}\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Erro ao salvar a figura: {e}\")\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "# Plot test data vs prediction\n",
    "# def plot_future(predictionGRU, predictionLSTM, y_test, link):\n",
    "#     plt.figure(figsize=(15, 6), dpi=100)\n",
    "#     range_future = len(y_test)\n",
    "#     plt.plot(np.arange(range_future), np.array(y_test), label='Test data')\n",
    "#     plt.plot(np.arange(range_future), np.array(predictionGRU), label='GRU')\n",
    "#     plt.plot(np.arange(range_future), np.array(predictionLSTM), label='LSTM')\n",
    "#     # dict_to_dataframe_prediction = {\n",
    "#     #     # \"range_future\": np.arange(range_future),\n",
    "#     #     f\"prediction{model_name}\": np.array(prediction.squeeze())\n",
    "#     # }\n",
    "    \n",
    "#     plt.title('Test data vs prediction for '+ link)\n",
    "#     plt.legend(loc='upper left')\n",
    "#     plt.xlabel('Time')\n",
    "#     plt.ylabel('Mbis/s')\n",
    "\n",
    "#     #Tenta salvar a fig\n",
    "#     save_path = '../../graficos/predicoes/round_2/graficos/' + link + '.png'\n",
    "#     try:\n",
    "#         plt.savefig(save_path)\n",
    "#         print(f\"A figura foi salva com sucesso em: {save_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erro ao salvar a figura: {e}\")\n",
    "\n",
    "#     plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "TRAINING_OUTPUT = os.path.join('training_output2.txt')\n",
    "THROUGHPUT_DATASETS = os.path.join('..', '..', 'datasets', 'test-recursive-lstm-test')\n",
    "MODEL = os.path.join(\"..\", \"..\", 'modelo_salvo')\n",
    "METRICS = os.path.join('..', '..', 'results', 'bi-lstm', 'evaluation_rmse_mae_2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para salvar o modelo\n",
    "def save_model(model, directory, substring_desejada, modelo):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    file_path = os.path.join(directory, f'{substring_desejada +modelo} - final_model.keras')\n",
    "    model.save(file_path)\n",
    "    print(f\"Modelo salvo como '{file_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main\n",
    "#### Model training and predcition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funtional with part of new solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_validation_hybrid(model, scaler, train_data, test_data, mask_test, look_back, window_size):\n",
    "    \"\"\"\n",
    "    Executa walk-forward validation com imputação híbrida preservando estado temporal\n",
    "    \n",
    "    Parâmetros:\n",
    "    model -- Modelo LSTM pré-treinado\n",
    "    scaler -- Scaler usado na normalização\n",
    "    train_data -- Dados de treino normalizados\n",
    "    test_data -- Dados de teste normalizados\n",
    "    mask_test -- Máscara de dados faltantes no teste\n",
    "    look_back -- Tamanho da janela histórica\n",
    "    window_size -- Tamanho da janela de predição\n",
    "    \n",
    "    Retorna:\n",
    "    predictions -- Lista de previsões em escala original\n",
    "    \"\"\"\n",
    "    # Estado inicial: últimos pontos do treino\n",
    "    state = train_data[-look_back:].reshape(1, look_back, 1)\n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(0, len(test_data), window_size):\n",
    "        # Seleciona janela atual\n",
    "        end_idx = min(i + window_size, len(test_data))\n",
    "        window_data = test_data[i:end_idx]\n",
    "        window_mask = mask_test[i:end_idx]\n",
    "        \n",
    "        window_preds = []\n",
    "        current_state = state\n",
    "        \n",
    "        # Predição passo-a-passo dentro da janela\n",
    "        for j in range(len(window_data)):\n",
    "            # Faz predição com estado atual\n",
    "            pred_scaled = model.predict(current_state, verbose=0)\n",
    "            pred_orig = scaler.inverse_transform(pred_scaled)[0,0]\n",
    "            window_preds.append(pred_orig)\n",
    "            \n",
    "            # Atualiza estado com dado real ou predição\n",
    "            if window_mask[j]:\n",
    "                new_point = window_data[j]\n",
    "            else:\n",
    "                new_point = pred_scaled[0,0]\n",
    "            \n",
    "            # Atualiza estado: remove ponto mais antigo, adiciona novo\n",
    "            current_state = np.roll(current_state, -1, axis=1)\n",
    "            current_state[0, -1, 0] = new_point\n",
    "        \n",
    "        predictions.extend(window_preds)\n",
    "        state = current_state  # Mantém estado para próxima janela\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Certifique-se de importar funções auxiliares usadas no script\n",
    "# from utils import bits_para_megabits, linear_interpolation, create_dataset, grid_search_cv\n",
    "# from utils import create_lstm, walk_forward_validation_hybrid, validate_missing_data_prediction\n",
    "# from utils import save_model, calculate_mean_history\n",
    "\n",
    "# Variáveis globais/configuradas\n",
    "LOOK_BACK = 3\n",
    "WINDOW_SIZE = 28  # Tamanho da janela de predição\n",
    "# TRAINING_OUTPUT = 'training_output.txt'  # Ajuste conforme necessário\n",
    "# THROUGHPUT_DATASETS = './datasets'  # Caminho correto\n",
    "# MODEL = './saved_models'  # Caminho correto para salvar os modelos\n",
    "\n",
    "# Funções de plot\n",
    "\n",
    "def plot_loss(history, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history['loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_predictions(y_true, y_pred, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_true, label='True Values')\n",
    "    plt.plot(y_pred, label='Predictions')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Throughput (Mbps)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Redirecionar saída padrão\n",
    "orig_stdout = sys.stdout\n",
    "\n",
    "with open(TRAINING_OUTPUT, 'w', encoding='utf-8') as f:\n",
    "    sys.stdout = f\n",
    "\n",
    "    diretorio_raiz = THROUGHPUT_DATASETS\n",
    "    evaluation = {}\n",
    "\n",
    "    for pasta_raiz, subpastas, arquivos in os.walk(diretorio_raiz):\n",
    "        for arquivo in arquivos:\n",
    "            if arquivo.endswith('.csv'):\n",
    "                caminho_arquivo = os.path.join(pasta_raiz, arquivo)\n",
    "                try:\n",
    "                    partes = caminho_arquivo.split(os.sep)\n",
    "                    if len(partes) >= 6:\n",
    "                        substring_desejada = partes[4] + ' - ' + partes[5]\n",
    "                    else:\n",
    "                        substring_desejada = arquivo.replace('.csv', '')\n",
    "\n",
    "                    # Carga dos dados\n",
    "                    df = pd.read_csv(caminho_arquivo, index_col='Timestamp')\n",
    "\n",
    "                    # Remover colunas desnecessárias, se houver\n",
    "                    if '0' in df.columns:\n",
    "                        df.drop('0', axis=1, inplace=True)\n",
    "\n",
    "                    # Pré-processamento\n",
    "                    bits_para_megabits(df, 'Throughput')\n",
    "\n",
    "                    # Criar máscara ANTES de qualquer manipulação\n",
    "                    mask_total = ~(df['Throughput'].isna() | (df['Throughput'] == -1))\n",
    "\n",
    "                    # Split treino-teste\n",
    "                    train_size = int(len(df.index) * 0.8)\n",
    "                    train_data = df.iloc[:train_size].copy()\n",
    "                    test_data = df.iloc[train_size:].copy()\n",
    "\n",
    "                    # Interpolação APENAS no treino\n",
    "                    train_data = linear_interpolation(train_data)\n",
    "\n",
    "                    # Normalização\n",
    "                    scaler = MinMaxScaler().fit(train_data[['Throughput']])\n",
    "                    train_scaled = scaler.transform(train_data[['Throughput']])\n",
    "                    test_scaled = scaler.transform(test_data[['Throughput']])\n",
    "\n",
    "                    mask_test = mask_total.iloc[train_size:].values\n",
    "\n",
    "                    # Treinamento do modelo\n",
    "                    X_train, y_train = create_dataset(train_scaled, LOOK_BACK)\n",
    "\n",
    "                    best_params = grid_search_cv(\n",
    "                        create_lstm, 64, X_train, [1e-3],\n",
    "                        y_train, [100,300,500], [32, 64, 128], [10], 'LSTM'\n",
    "                    )\n",
    "\n",
    "                    # Treinar modelo final\n",
    "                    model = create_lstm(64, X_train, best_params['learning_rate'])\n",
    "                    history = model.fit(\n",
    "                        X_train, y_train,\n",
    "                        epochs=best_params['epochs'],\n",
    "                        batch_size=best_params['batch_size'],\n",
    "                        validation_split=0.2,\n",
    "                        callbacks=[EarlyStopping(\n",
    "                            monitor='val_loss',\n",
    "                            patience=best_params['patience'],\n",
    "                            restore_best_weights=True\n",
    "                        )]\n",
    "                    )\n",
    "\n",
    "                    # Predição walk-forward\n",
    "                    predictions = walk_forward_validation_hybrid(\n",
    "                        model, scaler, train_scaled, test_scaled,\n",
    "                        mask_test, LOOK_BACK, WINDOW_SIZE\n",
    "                    )\n",
    "\n",
    "                    # Avaliação\n",
    "                    min_len = min(len(predictions), len(test_scaled) - LOOK_BACK)\n",
    "                    y_test_valid = test_scaled[LOOK_BACK:LOOK_BACK + min_len]\n",
    "                    mask_eval = mask_test[LOOK_BACK:LOOK_BACK + min_len]\n",
    "\n",
    "                    y_test_orig = scaler.inverse_transform(y_test_valid)\n",
    "                    predictions_arr = np.array(predictions[:min_len]).reshape(-1, 1)\n",
    "\n",
    "                    lstm_eval = validate_missing_data_prediction(\n",
    "                        predictions_arr, y_test_orig, mask_eval, 'LSTM'\n",
    "                    )\n",
    "\n",
    "                    evaluation[f\"{substring_desejada}, {lstm_eval[3]}\"] = lstm_eval[:3]\n",
    "\n",
    "                    print(f\"RMSE para {substring_desejada}: {lstm_eval[0]:.4f}\")\n",
    "                    print(f\"MAE para {substring_desejada}: {lstm_eval[1]:.4f}\")\n",
    "                    print(f\"NRMSE para {substring_desejada}: {lstm_eval[3]:.4f}\")\n",
    "\n",
    "                    # Salvar e plotar\n",
    "                    save_model(model, MODEL, substring_desejada, 'LSTM')\n",
    "                    plot_loss(history.history, f'LSTM Loss - {substring_desejada}')\n",
    "                    plot_predictions(y_test_orig, predictions_arr, f'LSTM Predictions - {substring_desejada}')\n",
    "                    \n",
    "                    import json  # Certifique-se de importar o json se for usá-lo\n",
    "\n",
    "                    # Diretório para salvar predições\n",
    "                    PREDICTIONS = './saved_predictions'\n",
    "                    os.makedirs(PREDICTIONS, exist_ok=True)\n",
    "\n",
    "                    # Salvar predições e valores reais em CSV\n",
    "                    try:\n",
    "                        pred_df = pd.DataFrame({\n",
    "                            'True_Throughput': y_test_orig.flatten(),\n",
    "                            'Predicted_Throughput': predictions_arr.flatten()\n",
    "                        })\n",
    "\n",
    "                        nome_base = substring_desejada.replace(' ', '_').replace('/', '_')\n",
    "                        pred_path = os.path.join(PREDICTIONS, f'{nome_base}_predictions.csv')\n",
    "                        pred_df.to_csv(pred_path, index=False)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Não foi possível salvar predições para {substring_desejada}: {str(e)}\")\n",
    "\n",
    "                    novo_dicionario = {}\n",
    "                    \n",
    "                    # Itere pelo dicionário original\n",
    "                    for chave, valores in lstm_eval.items():\n",
    "                        valor1, valor2, valor3 = valores  # Desempacote os valores da tupla\n",
    "                        novo_dicionario[chave] = {'RMSE': valor1, 'MAE': valor2, 'NRMSE': valor3}\n",
    "\n",
    "                    try:\n",
    "                        with open(METRICS, 'w', encoding='utf-8') as f:\n",
    "                            json.dump(novo_dicionario, f, indent=4)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Não foi possível salvar resultados em {METRICS}: {str(e)}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro no arquivo {arquivo}: {str(e)}\")\n",
    "\n",
    "    # Restaurar stdout\n",
    "    sys.stdout = orig_stdout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "import tensorflow.keras as keras\n",
    "import random\n",
    "\n",
    "# ===================================================================================\n",
    "# FUNÇÕES PARA SIMULAÇÃO DE FALHAS E CRIAÇÃO DE FEATURES AUXILIARES\n",
    "# ===================================================================================\n",
    "\n",
    "def identify_low_missing_regions(data, mask, min_consecutive=10, max_missing_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Identifica regiões com poucas falhas para simulação controlada\n",
    "    \n",
    "    Parâmetros:\n",
    "    data -- dados originais\n",
    "    mask -- máscara de dados válidos (True = válido, False = faltante)\n",
    "    min_consecutive -- mínimo de pontos consecutivos válidos\n",
    "    max_missing_ratio -- máximo ratio de dados faltantes permitido na região\n",
    "    \n",
    "    Retorna:\n",
    "    regions -- lista de (start_idx, end_idx) das regiões identificadas\n",
    "    \"\"\"\n",
    "    regions = []\n",
    "    window_size = max(50, min_consecutive * 2)  # Janela adaptativa\n",
    "    \n",
    "    for i in range(0, len(data) - window_size, window_size // 2):\n",
    "        window_mask = mask[i:i + window_size]\n",
    "        missing_ratio = 1 - np.mean(window_mask)\n",
    "        \n",
    "        if missing_ratio <= max_missing_ratio:\n",
    "            # Verifica se há sequências consecutivas suficientes\n",
    "            consecutive_count = 0\n",
    "            max_consecutive = 0\n",
    "            \n",
    "            for valid in window_mask:\n",
    "                if valid:\n",
    "                    consecutive_count += 1\n",
    "                    max_consecutive = max(max_consecutive, consecutive_count)\n",
    "                else:\n",
    "                    consecutive_count = 0\n",
    "            \n",
    "            if max_consecutive >= min_consecutive:\n",
    "                regions.append((i, i + window_size))\n",
    "    \n",
    "    return regions\n",
    "\n",
    "def simulate_missing_data(data, mask, missing_ratio=0.2, seed=None):\n",
    "    \"\"\"\n",
    "    Simula dados faltantes artificialmente em regiões com poucos gaps\n",
    "    \n",
    "    Parâmetros:\n",
    "    data -- dados originais\n",
    "    mask -- máscara original de dados válidos\n",
    "    missing_ratio -- percentual de dados a serem removidos artificialmente\n",
    "    seed -- seed para reproducibilidade\n",
    "    \n",
    "    Retorna:\n",
    "    artificial_mask -- nova máscara com falhas artificiais\n",
    "    artificial_data -- dados com falhas artificiais\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    # Identifica regiões com poucas falhas\n",
    "    regions = identify_low_missing_regions(data, mask)\n",
    "    \n",
    "    if not regions:\n",
    "        print(\"Aviso: Nenhuma região com poucos gaps encontrada\")\n",
    "        return mask.copy(), data.copy()\n",
    "    \n",
    "    artificial_mask = mask.copy()\n",
    "    artificial_data = data.copy()\n",
    "    \n",
    "    # Para cada região, remove dados artificialmente\n",
    "    for start_idx, end_idx in regions:\n",
    "        region_mask = mask[start_idx:end_idx]\n",
    "        valid_indices = np.where(region_mask)[0]\n",
    "        \n",
    "        if len(valid_indices) > 0:\n",
    "            # Remove uma porcentagem dos dados válidos\n",
    "            n_to_remove = int(len(valid_indices) * missing_ratio)\n",
    "            indices_to_remove = np.random.choice(valid_indices, n_to_remove, replace=False)\n",
    "            \n",
    "            for idx in indices_to_remove:\n",
    "                artificial_mask[start_idx + idx] = False\n",
    "                artificial_data[start_idx + idx] = np.nan\n",
    "    \n",
    "    return artificial_mask, artificial_data\n",
    "\n",
    "def create_auxiliary_features(data, mask):\n",
    "    \"\"\"\n",
    "    Cria features auxiliares: máscara e tempo desde última observação válida\n",
    "    \n",
    "    Parâmetros:\n",
    "    data -- série temporal\n",
    "    mask -- máscara de dados válidos\n",
    "    \n",
    "    Retorna:\n",
    "    mask_feature -- feature binária (1=real, 0=imputado)\n",
    "    delta_t_feature -- tempo desde última observação real\n",
    "    \"\"\"\n",
    "    mask_feature = mask.astype(int)\n",
    "    delta_t_feature = np.zeros_like(mask, dtype=float)\n",
    "    \n",
    "    last_valid_idx = -1\n",
    "    for i in range(len(mask)):\n",
    "        if mask[i]:\n",
    "            last_valid_idx = i\n",
    "            delta_t_feature[i] = 0\n",
    "        else:\n",
    "            if last_valid_idx >= 0:\n",
    "                delta_t_feature[i] = i - last_valid_idx\n",
    "            else:\n",
    "                delta_t_feature[i] = i + 1  # Desde o início\n",
    "    \n",
    "    # Normaliza delta_t\n",
    "    max_delta = np.max(delta_t_feature)\n",
    "    if max_delta > 0:\n",
    "        delta_t_feature = delta_t_feature / max_delta\n",
    "    \n",
    "    return mask_feature, delta_t_feature\n",
    "\n",
    "def create_dataset_with_auxiliary_features(X, mask_feature, delta_t_feature, look_back=3):\n",
    "    \"\"\"\n",
    "    Cria dataset com features auxiliares para LSTM\n",
    "    \n",
    "    Parâmetros:\n",
    "    X -- dados principais\n",
    "    mask_feature -- feature de máscara\n",
    "    delta_t_feature -- feature de tempo\n",
    "    look_back -- janela histórica\n",
    "    \n",
    "    Retorna:\n",
    "    Xs -- entradas com 3 features: [valor, mask, delta_t]\n",
    "    ys -- saídas\n",
    "    mask_ys -- máscara das saídas\n",
    "    \"\"\"\n",
    "    Xs, ys, mask_ys = [], [], []\n",
    "    \n",
    "    for i in range(len(X) - look_back):\n",
    "        # Features principais\n",
    "        v_main = X[i:i+look_back].reshape(-1, 1)\n",
    "        v_mask = mask_feature[i:i+look_back].reshape(-1, 1)\n",
    "        v_delta = delta_t_feature[i:i+look_back].reshape(-1, 1)\n",
    "        \n",
    "        # Combina features\n",
    "        v = np.concatenate([v_main, v_mask, v_delta], axis=1)\n",
    "        \n",
    "        Xs.append(v)\n",
    "        ys.append(X[i+look_back])\n",
    "        mask_ys.append(mask_feature[i+look_back])\n",
    "        \n",
    "    return np.array(Xs), np.array(ys), np.array(mask_ys)\n",
    "\n",
    "# ===================================================================================\n",
    "# MODELO LSTM MELHORADO COM FEATURES AUXILIARES\n",
    "# ===================================================================================\n",
    "\n",
    "def create_robust_lstm(units, train_shape, learning_rate, dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Cria modelo LSTM robusto com features auxiliares\n",
    "    \n",
    "    Parâmetros:\n",
    "    units -- número de unidades LSTM\n",
    "    train_shape -- formato dos dados de treino\n",
    "    learning_rate -- taxa de aprendizado\n",
    "    dropout_rate -- taxa de dropout\n",
    "    \n",
    "    Retorna:\n",
    "    model -- modelo LSTM compilado\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Primeira camada LSTM com dropout\n",
    "    model.add(LSTM(units=units, return_sequences=True, \n",
    "                   input_shape=[train_shape[1], train_shape[2]]))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Segunda camada LSTM\n",
    "    model.add(LSTM(units=units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Camada de saída\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compilação com loss Huber (mais robusto a outliers)\n",
    "    model.compile(\n",
    "        loss=Huber(delta=0.5),\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        metrics=[RootMeanSquaredError()]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def weighted_loss_function(y_true, y_pred, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Função de perda ponderada baseada na máscara\n",
    "    \"\"\"\n",
    "    if sample_weight is None:\n",
    "        return keras.losses.huber(y_true, y_pred)\n",
    "    else:\n",
    "        loss = keras.losses.huber(y_true, y_pred)\n",
    "        return loss * sample_weight\n",
    "\n",
    "# ===================================================================================\n",
    "# GRID SEARCH COM VALIDAÇÃO CRUZADA MELHORADA\n",
    "# ===================================================================================\n",
    "\n",
    "def grid_search_cv_robust(modelo, units, X_train, learning_rates, y_train, mask_train,\n",
    "                         epochs_list, batch_sizes, patiences, model_name):\n",
    "    \"\"\"\n",
    "    Grid search com validação cruzada considerando dados faltantes\n",
    "    \"\"\"\n",
    "    best_loss = float('inf')\n",
    "    best_params = {}\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        for epochs in epochs_list:\n",
    "            for batch_size in batch_sizes:\n",
    "                for patience in patiences:\n",
    "                    model = modelo(units, X_train, lr)\n",
    "                    histories = fit_model_with_cross_validation_robust(\n",
    "                        model, X_train, y_train, mask_train, model_name, \n",
    "                        patience, epochs, batch_size\n",
    "                    )\n",
    "                    mean_history = calculate_mean_history(histories)\n",
    "                    val_loss = min(mean_history['val_loss'])\n",
    "                    \n",
    "                    print(f\"Val Loss: {val_loss:.4f}, lr: {lr}, epochs: {epochs}, \"\n",
    "                          f\"batch_size: {batch_size}, patience: {patience}\")\n",
    "                    \n",
    "                    if val_loss < best_loss:\n",
    "                        best_loss = val_loss\n",
    "                        best_params = {\n",
    "                            'learning_rate': lr, \n",
    "                            'epochs': epochs, \n",
    "                            'batch_size': batch_size, \n",
    "                            'patience': patience\n",
    "                        }\n",
    "    \n",
    "    print(f'Melhores parâmetros para {model_name}: {best_params}')\n",
    "    return best_params\n",
    "\n",
    "def fit_model_with_cross_validation_robust(model, xtrain, ytrain, mask_train, \n",
    "                                          model_name, patience, epochs, batch_size):\n",
    "    \"\"\"\n",
    "    Validação cruzada com pesos baseados na máscara\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    fold = 1\n",
    "    histories = []\n",
    "    \n",
    "    for train_index, val_index in tscv.split(xtrain):\n",
    "        x_train_fold, x_val_fold = xtrain[train_index], xtrain[val_index]\n",
    "        y_train_fold, y_val_fold = ytrain[train_index], ytrain[val_index]\n",
    "        mask_train_fold = mask_train[train_index]\n",
    "        mask_val_fold = mask_train[val_index]\n",
    "        \n",
    "        # Pesos baseados na máscara (dados reais têm peso maior)\n",
    "        sample_weights = np.where(mask_train_fold, 1.0, 0.3)\n",
    "        val_sample_weights = np.where(mask_val_fold, 1.0, 0.3)\n",
    "        \n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=patience, \n",
    "            restore_best_weights=True, \n",
    "            min_delta=1e-5\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            x_train_fold, y_train_fold,\n",
    "            epochs=epochs,\n",
    "            validation_data=(x_val_fold, y_val_fold, val_sample_weights),\n",
    "            batch_size=batch_size,\n",
    "            sample_weight=sample_weights,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(f'\\nTREINAMENTO - Fold {fold} do modelo: {model_name}')\n",
    "        histories.append(history)\n",
    "        fold += 1\n",
    "    \n",
    "    return histories\n",
    "\n",
    "def calculate_mean_history(histories):\n",
    "    \"\"\"\n",
    "    Calcula média dos históricos de treinamento\n",
    "    \"\"\"\n",
    "    mean_history = {\n",
    "        'loss': [], \n",
    "        'root_mean_squared_error': [], \n",
    "        'val_loss': [], \n",
    "        'val_root_mean_squared_error': []\n",
    "    }\n",
    "    \n",
    "    for fold_history in histories:\n",
    "        for key in mean_history.keys():\n",
    "            if key in fold_history.history:\n",
    "                mean_history[key].append(fold_history.history[key])\n",
    "    \n",
    "    # Equaliza comprimentos\n",
    "    for key, values in mean_history.items():\n",
    "        if values:\n",
    "            max_len = max(len(val) for val in values)\n",
    "            for i in range(len(values)):\n",
    "                if len(values[i]) < max_len:\n",
    "                    values[i] += [values[i][-1]] * (max_len - len(values[i]))\n",
    "    \n",
    "    # Calcula médias\n",
    "    for key, values in mean_history.items():\n",
    "        if values:\n",
    "            mean_history[key] = [sum(vals) / len(vals) for vals in zip(*values)]\n",
    "    \n",
    "    return mean_history\n",
    "\n",
    "# ===================================================================================\n",
    "# WALK-FORWARD VALIDATION MELHORADO\n",
    "# ===================================================================================\n",
    "\n",
    "def walk_forward_validation_robust(model, scaler, train_data, test_data, mask_test,\n",
    "                                  mask_feature_test, delta_t_feature_test,\n",
    "                                  look_back, window_size):\n",
    "    \"\"\"\n",
    "    Walk-forward validation robusto com features auxiliares\n",
    "    \"\"\"\n",
    "    # Preparar dados de entrada com features auxiliares\n",
    "    # Estado inicial: últimos pontos do treino\n",
    "    state_main = train_data[-look_back:]\n",
    "    state_mask = np.ones(look_back)  # Assume que dados de treino são válidos\n",
    "    state_delta = np.zeros(look_back)\n",
    "    \n",
    "    # Combinar features do estado inicial\n",
    "    state = np.stack([state_main, state_mask, state_delta], axis=1).reshape(1, look_back, 3)\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(0, len(test_data), window_size):\n",
    "        end_idx = min(i + window_size, len(test_data))\n",
    "        window_data = test_data[i:end_idx]\n",
    "        window_mask = mask_test[i:end_idx]\n",
    "        window_mask_feature = mask_feature_test[i:end_idx]\n",
    "        window_delta_feature = delta_t_feature_test[i:end_idx]\n",
    "        \n",
    "        window_preds = []\n",
    "        current_state = state\n",
    "        \n",
    "        for j in range(len(window_data)):\n",
    "            # Predição com estado atual\n",
    "            pred_scaled = model.predict(current_state, verbose=0)\n",
    "            pred_orig = scaler.inverse_transform(pred_scaled.reshape(-1, 1))[0, 0]\n",
    "            window_preds.append(pred_orig)\n",
    "            \n",
    "            # Atualizar estado\n",
    "            if window_mask[j]:\n",
    "                new_point_main = window_data[j]\n",
    "            else:\n",
    "                new_point_main = pred_scaled[0, 0]\n",
    "            \n",
    "            new_point_mask = window_mask_feature[j]\n",
    "            new_point_delta = window_delta_feature[j]\n",
    "            \n",
    "            # Atualizar estado: shift e adicionar novo ponto\n",
    "            current_state = np.roll(current_state, -1, axis=1)\n",
    "            current_state[0, -1, 0] = new_point_main\n",
    "            current_state[0, -1, 1] = new_point_mask\n",
    "            current_state[0, -1, 2] = new_point_delta\n",
    "        \n",
    "        predictions.extend(window_preds)\n",
    "        state = current_state\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# ===================================================================================\n",
    "# AVALIAÇÃO COM MÚLTIPLAS SIMULAÇÕES\n",
    "# ===================================================================================\n",
    "\n",
    "def evaluate_robustness_multiple_simulations(data, mask, n_simulations=10, \n",
    "                                           missing_ratios=[0.1, 0.2, 0.3]):\n",
    "    \"\"\"\n",
    "    Avalia robustez com múltiplas simulações de dados faltantes\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for missing_ratio in missing_ratios:\n",
    "        rmse_list = []\n",
    "        mae_list = []\n",
    "        \n",
    "        for sim in range(n_simulations):\n",
    "            # Simula dados faltantes\n",
    "            artificial_mask, artificial_data = simulate_missing_data(\n",
    "                data, mask, missing_ratio, seed=sim\n",
    "            )\n",
    "            \n",
    "            # Aqui você executaria todo o pipeline de treinamento\n",
    "            # Por simplicidade, vou apenas simular métricas\n",
    "            # Na implementação real, você substituiria por:\n",
    "            # rmse, mae = train_and_evaluate_model(artificial_data, artificial_mask)\n",
    "            \n",
    "            # Simulação de métricas (substituir por código real)\n",
    "            rmse = np.random.normal(0.5, 0.1)  # Exemplo\n",
    "            mae = np.random.normal(0.3, 0.05)  # Exemplo\n",
    "            \n",
    "            rmse_list.append(rmse)\n",
    "            mae_list.append(mae)\n",
    "        \n",
    "        results[missing_ratio] = {\n",
    "            'rmse_mean': np.mean(rmse_list),\n",
    "            'rmse_std': np.std(rmse_list),\n",
    "            'mae_mean': np.mean(mae_list),\n",
    "            'mae_std': np.std(mae_list)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ===================================================================================\n",
    "# EXEMPLO DE USO INTEGRADO\n",
    "# ===================================================================================\n",
    "\n",
    "def processo_completo_robusto(df, substring_desejada):\n",
    "    \"\"\"\n",
    "    Exemplo de como integrar todas as melhorias\n",
    "    \"\"\"\n",
    "    # Pré-processamento\n",
    "    mask_total = ~(df['Throughput'].isna() | (df['Throughput'] == -1))\n",
    "    \n",
    "    # Split treino-teste\n",
    "    train_size = int(len(df.index) * 0.8)\n",
    "    train_data = df.iloc[:train_size].copy()\n",
    "    test_data = df.iloc[train_size:].copy()\n",
    "    \n",
    "    # Criar máscara para teste\n",
    "    mask_test = mask_total.iloc[train_size:].values\n",
    "    \n",
    "    # Simular dados faltantes no treino para melhor robustez\n",
    "    train_mask = mask_total.iloc[:train_size].values\n",
    "    artificial_mask, artificial_data = simulate_missing_data(\n",
    "        train_data['Throughput'].values, train_mask, missing_ratio=0.15\n",
    "    )\n",
    "    \n",
    "    # Aplicar interpolação nos dados com falhas artificiais\n",
    "    train_data_artificial = train_data.copy()\n",
    "    train_data_artificial['Throughput'] = artificial_data\n",
    "    # linear_interpolation(train_data_artificial)  # Sua função de interpolação\n",
    "    \n",
    "    # Criar features auxiliares\n",
    "    mask_feature_train, delta_t_feature_train = create_auxiliary_features(\n",
    "        train_data_artificial['Throughput'].values, artificial_mask\n",
    "    )\n",
    "    mask_feature_test, delta_t_feature_test = create_auxiliary_features(\n",
    "        test_data['Throughput'].values, mask_test\n",
    "    )\n",
    "    \n",
    "    # Normalização\n",
    "    scaler = MinMaxScaler().fit(train_data_artificial[['Throughput']])\n",
    "    train_scaled = scaler.transform(train_data_artificial[['Throughput']])\n",
    "    test_scaled = scaler.transform(test_data[['Throughput']])\n",
    "    \n",
    "    # Criar datasets com features auxiliares\n",
    "    X_train, y_train, mask_y_train = create_dataset_with_auxiliary_features(\n",
    "        train_scaled.flatten(), mask_feature_train, delta_t_feature_train, look_back=3\n",
    "    )\n",
    "    \n",
    "    # Grid search robusto\n",
    "    best_params = grid_search_cv_robust(\n",
    "        create_robust_lstm, 64, X_train, [1e-3, 1e-4],\n",
    "        y_train, mask_y_train, [100, 200], [32, 64], [10], 'LSTM-Robust'\n",
    "    )\n",
    "    \n",
    "    # Treinar modelo final\n",
    "    model = create_robust_lstm(64, X_train.shape, best_params['learning_rate'])\n",
    "    \n",
    "    # Pesos para treinamento\n",
    "    sample_weights = np.where(mask_y_train, 1.0, 0.3)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=best_params['epochs'],\n",
    "        batch_size=best_params['batch_size'],\n",
    "        sample_weight=sample_weights,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=best_params['patience'],\n",
    "            restore_best_weights=True\n",
    "        )]\n",
    "    )\n",
    "    \n",
    "    # Predição robusta\n",
    "    predictions = walk_forward_validation_robust(\n",
    "        model, scaler, train_scaled.flatten(), test_scaled.flatten(),\n",
    "        mask_test, mask_feature_test, delta_t_feature_test,\n",
    "        look_back=3, window_size=28\n",
    "    )\n",
    "    \n",
    "    return model, predictions, history\n",
    "\n",
    "# ===================================================================================\n",
    "# MÉTRICAS DE AVALIAÇÃO MELHORADAS\n",
    "# ===================================================================================\n",
    "\n",
    "def validate_missing_data_prediction_robust(predictions, actual, mask, model_name):\n",
    "    \"\"\"\n",
    "    Versão melhorada da função de validação considerando incerteza\n",
    "    \"\"\"\n",
    "    predictions = predictions.flatten()\n",
    "    actual = actual.flatten()\n",
    "    mask = mask.astype(bool).flatten()\n",
    "    \n",
    "    # Avaliação apenas em pontos válidos\n",
    "    preds_valid = predictions[mask]\n",
    "    acts_valid = actual[mask]\n",
    "    \n",
    "    if len(acts_valid) == 0:\n",
    "        print(f\"{model_name}: Nenhum ponto válido para avaliação!\")\n",
    "        return np.nan, np.nan, np.nan, model_name\n",
    "    \n",
    "    # Métricas tradicionais\n",
    "    errors = preds_valid - acts_valid\n",
    "    mse = np.square(errors).mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.abs(errors).mean()\n",
    "    nrmse = rmse / (acts_valid.max() - acts_valid.min() + 1e-12)\n",
    "    \n",
    "    # Métricas adicionais de robustez\n",
    "    median_error = np.median(np.abs(errors))\n",
    "    q75_error = np.percentile(np.abs(errors), 75)\n",
    "    \n",
    "    print(f'{model_name} (robust evaluation):')\n",
    "    print(f'MAE: {mae:.4f}')\n",
    "    print(f'RMSE: {rmse:.4f}')\n",
    "    print(f'NRMSE: {nrmse*100:.4f}%')\n",
    "    print(f'Median Error: {median_error:.4f}')\n",
    "    print(f'75th Percentile Error: {q75_error:.4f}')\n",
    "    print(f'Valid points: {len(acts_valid)}/{len(actual)} ({len(acts_valid)/len(actual)*100:.1f}%)')\n",
    "    print('')\n",
    "    \n",
    "    return rmse, mae, nrmse, model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import json\n",
    "\n",
    "# Importar funções do código melhorado\n",
    "# from robust_lstm_functions import *  # Todas as funções do código anterior\n",
    "\n",
    "# Configurações\n",
    "BASE_DIR = \"resultados-recursive-prediction-robust\"\n",
    "PREDICTIONS_DIR = os.path.join(BASE_DIR, \"predicoes\")\n",
    "PLOTS_DIR = os.path.join(BASE_DIR, \"plots\")\n",
    "PLOTS_LOSS_DIR = os.path.join(PLOTS_DIR, \"loss\")\n",
    "PLOTS_PREDICTIONS_DIR = os.path.join(PLOTS_DIR, \"predicoes\")\n",
    "TRAINING_OUTPUT = os.path.join(BASE_DIR, 'training_output_robust.txt')\n",
    "METRICS_PATH = os.path.join(BASE_DIR, 'metrics_robust.json')\n",
    "\n",
    "# Criar diretórios\n",
    "for dir_path in [BASE_DIR, PREDICTIONS_DIR, PLOTS_DIR, PLOTS_LOSS_DIR, PLOTS_PREDICTIONS_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "LOOK_BACK = 3\n",
    "WINDOW_SIZE = 28\n",
    "TRAINING_OUTPUT = os.path.join('training_output2.txt')\n",
    "THROUGHPUT_DATASETS = os.path.join('..', '..', 'datasets', 'test-recursive-lstm-test')\n",
    "MODEL_DIR = './saved_models_robust'\n",
    "\n",
    "# Função principal melhorada\n",
    "def processo_arquivo_robusto(caminho_arquivo, arquivo):\n",
    "    \"\"\"\n",
    "    Processa um arquivo CSV com o pipeline robusto completo\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extrair nome do arquivo\n",
    "        partes = caminho_arquivo.split(os.sep)\n",
    "        if len(partes) >= 6:\n",
    "            substring_desejada = partes[4] + ' - ' + partes[5]\n",
    "        else:\n",
    "            substring_desejada = arquivo.replace('.csv', '')\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processando: {substring_desejada}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Carregar dados\n",
    "        df = pd.read_csv(caminho_arquivo, index_col='Timestamp')\n",
    "        \n",
    "        # Remover colunas desnecessárias\n",
    "        if '0' in df.columns:\n",
    "            df.drop('0', axis=1, inplace=True)\n",
    "        \n",
    "        # Pré-processamento\n",
    "        # bits_para_megabits(df, 'Throughput')  # Sua função\n",
    "        \n",
    "        # Criar máscara original\n",
    "        mask_total = ~(df['Throughput'].isna() | (df['Throughput'] == -1))\n",
    "        \n",
    "        # Verificar se há dados suficientes\n",
    "        if np.sum(mask_total) < 100:\n",
    "            print(f\"Dados insuficientes para {substring_desejada}\")\n",
    "            return None\n",
    "        \n",
    "        # Split treino-teste\n",
    "        train_size = int(len(df.index) * 0.8)\n",
    "        train_data = df.iloc[:train_size].copy()\n",
    "        test_data = df.iloc[train_size:].copy()\n",
    "        \n",
    "        # Máscaras\n",
    "        mask_train = mask_total.iloc[:train_size].values\n",
    "        mask_test = mask_total.iloc[train_size:].values\n",
    "        \n",
    "        print(f\"Dados de treino: {len(train_data)} pontos, {np.sum(mask_train)} válidos ({np.sum(mask_train)/len(mask_train)*100:.1f}%)\")\n",
    "        print(f\"Dados de teste: {len(test_data)} pontos, {np.sum(mask_test)} válidos ({np.sum(mask_test)/len(mask_test)*100:.1f}%)\")\n",
    "        \n",
    "        # ===================================================================================\n",
    "        # ESTRATÉGIA 1: SIMULAÇÃO DE FALHAS ARTIFICIAIS\n",
    "        # ===================================================================================\n",
    "        \n",
    "        print(\"\\n1. Simulando falhas artificiais no treino...\")\n",
    "        \n",
    "        # Identificar regiões com poucos gaps\n",
    "        regions = identify_low_missing_regions(train_data['Throughput'].values, mask_train)\n",
    "        print(f\"Regiões com poucos gaps identificadas: {len(regions)}\")\n",
    "        \n",
    "        # Simular falhas artificiais (15% dos dados válidos)\n",
    "        artificial_mask, artificial_data = simulate_missing_data(\n",
    "            train_data['Throughput'].values, mask_train, \n",
    "            missing_ratio=0.15, seed=42\n",
    "        )\n",
    "        \n",
    "        original_valid = np.sum(mask_train)\n",
    "        artificial_valid = np.sum(artificial_mask)\n",
    "        print(f\"Dados válidos: {original_valid} -> {artificial_valid} (removidos: {original_valid - artificial_valid})\")\n",
    "        \n",
    "        # Aplicar interpolação nos dados com falhas artificiais\n",
    "        train_data_artificial = train_data.copy()\n",
    "        train_data_artificial['Throughput'] = artificial_data\n",
    "        \n",
    "        # Interpolação linear simples (substitua pela sua função)\n",
    "        train_data_artificial['Throughput'] = train_data_artificial['Throughput'].interpolate(method='linear')\n",
    "        \n",
    "        # ===================================================================================\n",
    "        # ESTRATÉGIA 2: FEATURES AUXILIARES\n",
    "        # ===================================================================================\n",
    "        \n",
    "        print(\"\\n2. Criando features auxiliares...\")\n",
    "        \n",
    "        # Criar features auxiliares para treino\n",
    "        mask_feature_train, delta_t_feature_train = create_auxiliary_features(\n",
    "            train_data_artificial['Throughput'].values, artificial_mask\n",
    "        )\n",
    "        \n",
    "        # Criar features auxiliares para teste\n",
    "        test_data_interpolated = test_data.copy()\n",
    "        test_data_interpolated['Throughput'] = test_data_interpolated['Throughput'].interpolate(method='linear')\n",
    "        \n",
    "        mask_feature_test, delta_t_feature_test = create_auxiliary_features(\n",
    "            test_data_interpolated['Throughput'].values, mask_test\n",
    "        )\n",
    "        \n",
    "        print(f\"Features auxiliares criadas: mask + delta_t\")\n",
    "        \n",
    "        # ===================================================================================\n",
    "        # ESTRATÉGIA 3: NORMALIZAÇÃO E DATASET COM FEATURES AUXILIARES\n",
    "        # ===================================================================================\n",
    "        \n",
    "        print(\"\\n3. Preparando dados com features auxiliares...\")\n",
    "        \n",
    "        # Normalização\n",
    "        scaler = MinMaxScaler().fit(train_data_artificial[['Throughput']])\n",
    "        train_scaled = scaler.transform(train_data_artificial[['Throughput']])\n",
    "        test_scaled = scaler.transform(test_data_interpolated[['Throughput']])\n",
    "        \n",
    "        # Criar datasets com features auxiliares\n",
    "        X_train, y_train, mask_y_train = create_dataset_with_auxiliary_features(\n",
    "            train_scaled.flatten(), mask_feature_train, delta_t_feature_train, look_back=LOOK_BACK\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset de treino: {X_train.shape} (3 features: valor, mask, delta_t)\")\n",
    "        \n",
    "        # ===================================================================================\n",
    "        # ESTRATÉGIA 4: GRID SEARCH ROBUSTO\n",
    "        # ===================================================================================\n",
    "        \n",
    "        print(\"\\n4. Executando grid search robusto...\")\n",
    "        \n",
    "        best_params = grid_search_cv_robust(\n",
    "            create_robust_lstm, 64, X_train, [1e-3, 1e-4],\n",
    "            y_train, mask_y_train, [100, 200, 300], [32, 64], [10], 'LSTM-Robust'\n",
    "        )\n",
    "        \n",
    "        # ===================================================================================\n",
    "        # ESTRATÉGIA 5: TREINAMENTO COM LOSS PONDERADA\n",
    "        # ===================================================================================\n",
    "        \n",
    "        print(\"\\n5. Treinando modelo com loss ponderada...\")\n",
    "        \n",
    "        # Criar modelo final\n",
    "        model = create_robust_lstm(64, X_train.shape, 5)\n",
    "        \n",
    "                # Pesos para treinamento (dados reais têm peso maior)\n",
    "        sample_weights = np.where(mask_y_train, 1.0, 0.3)\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=best_params['patience'],\n",
    "            restore_best_weights=True,\n",
    "            min_delta=1e-5\n",
    "        )\n",
    "        \n",
    "        # Treinar modelo final\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=best_params['epochs'],\n",
    "            batch_size=best_params['batch_size'],\n",
    "            validation_split=0.2,\n",
    "            sample_weight=sample_weights,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # ===================================================================================\n",
    "        # ESTRATÉGIA 6: PREDIÇÃO WALK-FORWARD ROBUSTA\n",
    "        # ===================================================================================\n",
    "        \n",
    "        print(\"\\n6. Realizando predição walk-forward robusta...\")\n",
    "        \n",
    "        # Preparar dados de teste\n",
    "        test_scaled_flat = test_scaled.flatten()\n",
    "        \n",
    "        predictions = walk_forward_validation_robust(\n",
    "            model, scaler, \n",
    "            train_scaled.flatten(),  # Últimos pontos do treino\n",
    "            test_scaled_flat,\n",
    "            mask_test,\n",
    "            mask_feature_test,\n",
    "            delta_t_feature_test,\n",
    "            look_back=LOOK_BACK,\n",
    "            window_size=WINDOW_SIZE\n",
    "        )\n",
    "        \n",
    "        # Converter predições para formato original\n",
    "        predictions = np.array(predictions).reshape(-1, 1)\n",
    "        predictions_orig = scaler.inverse_transform(predictions)\n",
    "        \n",
    "        # ===================================================================================\n",
    "        # ESTRATÉGIA 7: AVALIAÇÃO ROBUSTA E SALVAMENTO DE RESULTADOS\n",
    "        # ===================================================================================\n",
    "        \n",
    "        print(\"\\n7. Avaliando e salvando resultados...\")\n",
    "        \n",
    "        # Validar predições considerando máscara\n",
    "        actual_values = test_data['Throughput'].values[LOOK_BACK:]\n",
    "        mask_valid = mask_test[LOOK_BACK:]\n",
    "        \n",
    "        rmse, mae, nrmse, _ = validate_missing_data_prediction_robust(\n",
    "            predictions_orig, \n",
    "            actual_values.reshape(-1, 1), \n",
    "            mask_valid,\n",
    "            substring_desejada\n",
    "        )\n",
    "        \n",
    "        # Salvar métricas\n",
    "        metrics = {\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'NRMSE': nrmse,\n",
    "            'Valid_Points': f\"{np.sum(mask_valid)}/{len(mask_valid)}\",\n",
    "            'Valid_Percentage': f\"{np.sum(mask_valid)/len(mask_valid)*100:.1f}%\"\n",
    "        }\n",
    "        \n",
    "        # Salvar predições\n",
    "        pred_df = pd.DataFrame({\n",
    "            'Timestamp': test_data.index[LOOK_BACK:],\n",
    "            'Actual': actual_values,\n",
    "            'Predicted': predictions_orig.flatten(),\n",
    "            'Valid': mask_valid\n",
    "        })\n",
    "        pred_file = os.path.join(PREDICTIONS_DIR, f\"{substring_desejada}_predictions.csv\")\n",
    "        pred_df.to_csv(pred_file, index=False)\n",
    "        \n",
    "        # Salvar modelo\n",
    "        model_file = os.path.join(MODEL_DIR, f\"{substring_desejada}_model.keras\")\n",
    "        model.save(model_file)\n",
    "        \n",
    "        # Salvar histórico de treinamento\n",
    "        history_file = os.path.join(BASE_DIR, f\"{substring_desejada}_history.json\")\n",
    "        with open(history_file, 'w') as f:\n",
    "            json.dump(history.history, f)\n",
    "            \n",
    "        # Plotar perda de treinamento\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'Training History: {substring_desejada}')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        loss_file = os.path.join(PLOTS_LOSS_DIR, f\"{substring_desejada}_loss.png\")\n",
    "        plt.savefig(loss_file)\n",
    "        plt.close()\n",
    "        \n",
    "        # Plotar predições vs reais\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        valid_indices = np.where(mask_valid)[0]\n",
    "        plt.plot(pred_df['Timestamp'], pred_df['Actual'], 'b-', label='Actual')\n",
    "        plt.plot(pred_df['Timestamp'], pred_df['Predicted'], 'r-', alpha=0.7, label='Predicted')\n",
    "        plt.scatter(\n",
    "            pred_df.iloc[valid_indices]['Timestamp'],\n",
    "            pred_df.iloc[valid_indices]['Actual'],\n",
    "            c='green', s=15, label='Valid Points'\n",
    "        )\n",
    "        plt.title(f'Prediction vs Actual: {substring_desejada}')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel('Throughput')\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        pred_file = os.path.join(PLOTS_PREDICTIONS_DIR, f\"{substring_desejada}_prediction.png\")\n",
    "        plt.savefig(pred_file)\n",
    "        plt.close()\n",
    "        \n",
    "        # Registrar métricas no arquivo de saída\n",
    "        with open(TRAINING_OUTPUT, 'a') as f:\n",
    "            f.write(f\"\\n\\n{'='*50}\\n\")\n",
    "            f.write(f\"Resultados para: {substring_desejada}\\n\")\n",
    "            f.write(f\"RMSE: {rmse:.4f}\\n\")\n",
    "            f.write(f\"MAE: {mae:.4f}\\n\")\n",
    "            f.write(f\"NRMSE: {nrmse*100:.2f}%\\n\")\n",
    "            f.write(f\"Valid Points: {np.sum(mask_valid)}/{len(mask_valid)} ({np.sum(mask_valid)/len(mask_valid)*100:.1f}%)\\n\")\n",
    "        \n",
    "        # Atualizar métricas globais\n",
    "        global_metrics = {}\n",
    "        if os.path.exists(METRICS_PATH):\n",
    "            with open(METRICS_PATH, 'r') as f:\n",
    "                global_metrics = json.load(f)\n",
    "                \n",
    "        global_metrics[substring_desejada] = metrics\n",
    "        \n",
    "        with open(METRICS_PATH, 'w') as f:\n",
    "            json.dump(global_metrics, f, indent=4)\n",
    "            \n",
    "        print(f\"Processamento completo para {substring_desejada}\")\n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro no processamento de {arquivo}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Função principal para executar o pipeline\n",
    "def main():\n",
    "    arquivos_processados = 0\n",
    "    \n",
    "    # Inicializar arquivo de saída\n",
    "    with open(TRAINING_OUTPUT, 'w') as f:\n",
    "        f.write(\"=== INÍCIO DO TREINAMENTO ROBUSTO ===\\n\")\n",
    "    \n",
    "    # Coletar todos os arquivos CSV\n",
    "    arquivos = []\n",
    "    for root, dirs, files in os.walk(THROUGHPUT_DATASETS):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                arquivos.append((os.path.join(root, file), file))\n",
    "    \n",
    "    # Processar cada arquivo\n",
    "    for caminho, arquivo in arquivos:\n",
    "        resultado = processo_arquivo_robusto(caminho, arquivo)\n",
    "        if resultado:\n",
    "            arquivos_processados += 1\n",
    "            \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processamento completo!\")\n",
    "    print(f\"Arquivos processados: {arquivos_processados}/{len(arquivos)}\")\n",
    "    print(f\"Resultados salvos em: {BASE_DIR}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 19:24:41.865577: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-03 19:24:41.865972: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-03 19:24:41.867794: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-03 19:24:41.872758: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751581481.881244 3199140 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751581481.883696 3199140 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751581481.890276 3199140 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751581481.890284 3199140 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751581481.890286 3199140 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751581481.890286 3199140 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-03 19:24:41.892765: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processando: test-failure - treated bbr esmond data ap-ba 07-03-2023.csv\n",
      "==================================================\n",
      "Dados de treino: 579 pontos, 545 válidos (94.1%)\n",
      "Dados de teste: 145 pontos, 78 válidos (53.8%)\n",
      "\n",
      "1. Simulando falhas artificiais no treino...\n",
      "Regiões com poucos gaps identificadas: 17\n",
      "Dados válidos: 545 -> 440 (removidos: 105)\n",
      "\n",
      "2. Criando features auxiliares...\n",
      "Features auxiliares criadas: mask + delta_t\n",
      "\n",
      "3. Preparando dados com features auxiliares...\n",
      "Dataset de treino: (576, 3, 3) (3 features: valor, mask, delta_t)\n",
      "\n",
      "4. Executando grid search robusto...\n",
      "Erro no processamento de treated bbr esmond data ap-ba 07-03-2023.csv: Argument `learning_rate` should be float, or an instance of LearningRateSchedule, or a callable (that takes in the current iteration value and returns the corresponding learning rate value). Received instead: learning_rate=5\n",
      "\n",
      "==================================================\n",
      "Processamento completo!\n",
      "Arquivos processados: 0/1\n",
      "Resultados salvos em: resultados-recursive-prediction-robust\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1751581482.621473 3199140 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1751581482.621668 3199140 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/home/clara/Documentos/recursive-prediction/.venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3199140/704871494.py\", line 488, in processo_arquivo_robusto\n",
      "    best_params = grid_search_cv_robust(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3199140/704871494.py\", line 177, in grid_search_cv_robust\n",
      "    model = modelo(units, input_shape, lr)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3199140/704871494.py\", line 149, in create_robust_lstm\n",
      "    optimizer=Adam(learning_rate=5),\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/clara/Documentos/recursive-prediction/.venv/lib/python3.12/site-packages/keras/src/optimizers/adam.py\", line 62, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/clara/Documentos/recursive-prediction/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 21, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"/home/clara/Documentos/recursive-prediction/.venv/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\", line 177, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Argument `learning_rate` should be float, or an instance of LearningRateSchedule, or a callable (that takes in the current iteration value and returns the corresponding learning rate value). Received instead: learning_rate=5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "import tensorflow.keras as keras\n",
    "import json\n",
    "import random\n",
    "\n",
    "# ===================================================================================\n",
    "# FUNÇÕES PARA SIMULAÇÃO DE FALHAS E CRIAÇÃO DE FEATURES AUXILIARES\n",
    "# ===================================================================================\n",
    "\n",
    "def identify_low_missing_regions(data, mask, min_consecutive=10, max_missing_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Identifica regiões com poucas falhas para simulação controlada\n",
    "    \"\"\"\n",
    "    regions = []\n",
    "    window_size = max(50, min_consecutive * 2)\n",
    "    \n",
    "    for i in range(0, len(data) - window_size, window_size // 2):\n",
    "        window_mask = mask[i:i + window_size]\n",
    "        missing_ratio = 1 - np.mean(window_mask)\n",
    "        \n",
    "        if missing_ratio <= max_missing_ratio:\n",
    "            consecutive_count = 0\n",
    "            max_consecutive = 0\n",
    "            \n",
    "            for valid in window_mask:\n",
    "                if valid:\n",
    "                    consecutive_count += 1\n",
    "                    max_consecutive = max(max_consecutive, consecutive_count)\n",
    "                else:\n",
    "                    consecutive_count = 0\n",
    "            \n",
    "            if max_consecutive >= min_consecutive:\n",
    "                regions.append((i, i + window_size))\n",
    "    \n",
    "    return regions\n",
    "\n",
    "def simulate_missing_data(data, mask, missing_ratio=0.2, seed=None):\n",
    "    \"\"\"\n",
    "    Simula dados faltantes artificialmente em regiões com poucos gaps\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    regions = identify_low_missing_regions(data, mask)\n",
    "    \n",
    "    if not regions:\n",
    "        print(\"Aviso: Nenhuma região com poucos gaps encontrada\")\n",
    "        return mask.copy(), data.copy()\n",
    "    \n",
    "    artificial_mask = mask.copy()\n",
    "    artificial_data = data.copy()\n",
    "    \n",
    "    for start_idx, end_idx in regions:\n",
    "        region_mask = mask[start_idx:end_idx]\n",
    "        valid_indices = np.where(region_mask)[0]\n",
    "        \n",
    "        if len(valid_indices) > 0:\n",
    "            n_to_remove = int(len(valid_indices) * missing_ratio)\n",
    "            indices_to_remove = np.random.choice(valid_indices, n_to_remove, replace=False)\n",
    "            \n",
    "            for idx in indices_to_remove:\n",
    "                artificial_mask[start_idx + idx] = False\n",
    "                artificial_data[start_idx + idx] = np.nan\n",
    "    \n",
    "    return artificial_mask, artificial_data\n",
    "\n",
    "def create_auxiliary_features(data, mask):\n",
    "    \"\"\"\n",
    "    Cria features auxiliares: máscara e tempo desde última observação válida\n",
    "    \"\"\"\n",
    "    mask_feature = mask.astype(int)\n",
    "    delta_t_feature = np.zeros_like(mask, dtype=float)\n",
    "    \n",
    "    last_valid_idx = -1\n",
    "    for i in range(len(mask)):\n",
    "        if mask[i]:\n",
    "            last_valid_idx = i\n",
    "            delta_t_feature[i] = 0\n",
    "        else:\n",
    "            if last_valid_idx >= 0:\n",
    "                delta_t_feature[i] = i - last_valid_idx\n",
    "            else:\n",
    "                delta_t_feature[i] = i + 1\n",
    "    \n",
    "    max_delta = np.max(delta_t_feature)\n",
    "    if max_delta > 0:\n",
    "        delta_t_feature = delta_t_feature / max_delta\n",
    "    \n",
    "    return mask_feature, delta_t_feature\n",
    "\n",
    "def create_dataset_with_auxiliary_features(X, mask_feature, delta_t_feature, look_back=3):\n",
    "    \"\"\"\n",
    "    Cria dataset com features auxiliares para LSTM\n",
    "    \"\"\"\n",
    "    Xs, ys, mask_ys = [], [], []\n",
    "    \n",
    "    for i in range(len(X) - look_back):\n",
    "        v_main = X[i:i+look_back].reshape(-1, 1)\n",
    "        v_mask = mask_feature[i:i+look_back].reshape(-1, 1)\n",
    "        v_delta = delta_t_feature[i:i+look_back].reshape(-1, 1)\n",
    "        \n",
    "        v = np.concatenate([v_main, v_mask, v_delta], axis=1)\n",
    "        \n",
    "        Xs.append(v)\n",
    "        ys.append(X[i+look_back])\n",
    "        mask_ys.append(mask_feature[i+look_back])\n",
    "        \n",
    "    return np.array(Xs), np.array(ys), np.array(mask_ys)\n",
    "\n",
    "# ===================================================================================\n",
    "# MODELO LSTM MELHORADO COM FEATURES AUXILIARES\n",
    "# ===================================================================================\n",
    "\n",
    "def create_robust_lstm(units, input_shape, learning_rate, dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Cria modelo LSTM robusto com features auxiliares\n",
    "    \n",
    "    CORREÇÃO: Agora recebe input_shape como tupla (timesteps, features)\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Primeira camada LSTM com dropout\n",
    "    model.add(LSTM(units=units, return_sequences=True, \n",
    "                   input_shape=input_shape))  # input_shape deve ser (timesteps, features)\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Segunda camada LSTM\n",
    "    model.add(LSTM(units=units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Camada de saída\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compilação com loss Huber\n",
    "    model.compile(\n",
    "        loss=Huber(delta=0.5),\n",
    "        optimizer=Adam(learning_rate=5),\n",
    "        metrics=[RootMeanSquaredError()]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ===================================================================================\n",
    "# GRID SEARCH COM VALIDAÇÃO CRUZADA MELHORADA\n",
    "# ===================================================================================\n",
    "\n",
    "def grid_search_cv_robust(modelo, units, X_train, learning_rates, y_train, mask_train,\n",
    "                         epochs_list, batch_sizes, patiences, model_name):\n",
    "    \"\"\"\n",
    "    Grid search com validação cruzada considerando dados faltantes\n",
    "    \n",
    "    CORREÇÃO: Passa input_shape corretamente\n",
    "    \"\"\"\n",
    "    best_loss = float('inf')\n",
    "    best_params = {}\n",
    "    \n",
    "    # Extrair shape dos dados de treino\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])  # (timesteps, features)\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        for epochs in epochs_list:\n",
    "            for batch_size in batch_sizes:\n",
    "                for patience in patiences:\n",
    "                    # CORREÇÃO: Passar input_shape ao invés de X_train\n",
    "                    model = modelo(units, input_shape, lr)\n",
    "                    histories = fit_model_with_cross_validation_robust(\n",
    "                        model, X_train, y_train, mask_train, model_name, \n",
    "                        patience, epochs, batch_size\n",
    "                    )\n",
    "                    mean_history = calculate_mean_history(histories)\n",
    "                    val_loss = min(mean_history['val_loss'])\n",
    "                    \n",
    "                    print(f\"Val Loss: {val_loss:.4f}, lr: {lr}, epochs: {epochs}, \"\n",
    "                          f\"batch_size: {batch_size}, patience: {patience}\")\n",
    "                    \n",
    "                    if val_loss < best_loss:\n",
    "                        best_loss = val_loss\n",
    "                        best_params = {\n",
    "                            'learning_rate': 5, \n",
    "                            'epochs': epochs, \n",
    "                            'batch_size': batch_size, \n",
    "                            'patience': patience\n",
    "                        }\n",
    "    \n",
    "    print(f'Melhores parâmetros para {model_name}: {best_params}')\n",
    "    return best_params\n",
    "\n",
    "def fit_model_with_cross_validation_robust(model, xtrain, ytrain, mask_train, \n",
    "                                          model_name, patience, epochs, batch_size):\n",
    "    \"\"\"\n",
    "    Validação cruzada com pesos baseados na máscara\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    fold = 1\n",
    "    histories = []\n",
    "    \n",
    "    for train_index, val_index in tscv.split(xtrain):\n",
    "        x_train_fold, x_val_fold = xtrain[train_index], xtrain[val_index]\n",
    "        y_train_fold, y_val_fold = ytrain[train_index], ytrain[val_index]\n",
    "        mask_train_fold = mask_train[train_index]\n",
    "        mask_val_fold = mask_train[val_index]\n",
    "        \n",
    "        # Pesos baseados na máscara\n",
    "        sample_weights = np.where(mask_train_fold, 1.0, 0.3)\n",
    "        val_sample_weights = np.where(mask_val_fold, 1.0, 0.3)\n",
    "        \n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=patience, \n",
    "            restore_best_weights=True, \n",
    "            min_delta=1e-5\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            x_train_fold, y_train_fold,\n",
    "            epochs=epochs,\n",
    "            validation_data=(x_val_fold, y_val_fold, val_sample_weights),\n",
    "            batch_size=batch_size,\n",
    "            sample_weight=sample_weights,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(f'\\nTREINAMENTO - Fold {fold} do modelo: {model_name}')\n",
    "        histories.append(history)\n",
    "        fold += 1\n",
    "    \n",
    "    return histories\n",
    "\n",
    "def calculate_mean_history(histories):\n",
    "    \"\"\"\n",
    "    Calcula média dos históricos de treinamento\n",
    "    \"\"\"\n",
    "    mean_history = {\n",
    "        'loss': [], \n",
    "        'root_mean_squared_error': [], \n",
    "        'val_loss': [], \n",
    "        'val_root_mean_squared_error': []\n",
    "    }\n",
    "    \n",
    "    for fold_history in histories:\n",
    "        for key in mean_history.keys():\n",
    "            if key in fold_history.history:\n",
    "                mean_history[key].append(fold_history.history[key])\n",
    "    \n",
    "    # Equaliza comprimentos\n",
    "    for key, values in mean_history.items():\n",
    "        if values:\n",
    "            max_len = max(len(val) for val in values)\n",
    "            for i in range(len(values)):\n",
    "                if len(values[i]) < max_len:\n",
    "                    values[i] += [values[i][-1]] * (max_len - len(values[i]))\n",
    "    \n",
    "    # Calcula médias\n",
    "    for key, values in mean_history.items():\n",
    "        if values:\n",
    "            mean_history[key] = [sum(vals) / len(vals) for vals in zip(*values)]\n",
    "    \n",
    "    return mean_history\n",
    "\n",
    "# ===================================================================================\n",
    "# WALK-FORWARD VALIDATION MELHORADO\n",
    "# ===================================================================================\n",
    "\n",
    "def walk_forward_validation_robust(model, scaler, train_data, test_data, mask_test,\n",
    "                                  mask_feature_test, delta_t_feature_test,\n",
    "                                  look_back, window_size):\n",
    "    \"\"\"\n",
    "    Walk-forward validation robusto com features auxiliares\n",
    "    \"\"\"\n",
    "    # Estado inicial: últimos pontos do treino\n",
    "    state_main = train_data[-look_back:]\n",
    "    state_mask = np.ones(look_back)\n",
    "    state_delta = np.zeros(look_back)\n",
    "    \n",
    "    # Combinar features do estado inicial\n",
    "    state = np.stack([state_main, state_mask, state_delta], axis=1).reshape(1, look_back, 3)\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(0, len(test_data), window_size):\n",
    "        end_idx = min(i + window_size, len(test_data))\n",
    "        window_data = test_data[i:end_idx]\n",
    "        window_mask = mask_test[i:end_idx]\n",
    "        window_mask_feature = mask_feature_test[i:end_idx]\n",
    "        window_delta_feature = delta_t_feature_test[i:end_idx]\n",
    "        \n",
    "        window_preds = []\n",
    "        current_state = state\n",
    "        \n",
    "        for j in range(len(window_data)):\n",
    "            # Predição com estado atual\n",
    "            pred_scaled = model.predict(current_state, verbose=0)\n",
    "            pred_orig = scaler.inverse_transform(pred_scaled.reshape(-1, 1))[0, 0]\n",
    "            window_preds.append(pred_orig)\n",
    "            \n",
    "            # Atualizar estado\n",
    "            if window_mask[j]:\n",
    "                new_point_main = window_data[j]\n",
    "            else:\n",
    "                new_point_main = pred_scaled[0, 0]\n",
    "            \n",
    "            new_point_mask = window_mask_feature[j]\n",
    "            new_point_delta = window_delta_feature[j]\n",
    "            \n",
    "            # Atualizar estado: shift e adicionar novo ponto\n",
    "            current_state = np.roll(current_state, -1, axis=1)\n",
    "            current_state[0, -1, 0] = new_point_main\n",
    "            current_state[0, -1, 1] = new_point_mask\n",
    "            current_state[0, -1, 2] = new_point_delta\n",
    "        \n",
    "        predictions.extend(window_preds)\n",
    "        state = current_state\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# ===================================================================================\n",
    "# AVALIAÇÃO COM MÚLTIPLAS MÉTRICAS\n",
    "# ===================================================================================\n",
    "\n",
    "def validate_missing_data_prediction_robust(predictions, actual, mask, model_name):\n",
    "    \"\"\"\n",
    "    Versão melhorada da função de validação considerando incerteza\n",
    "    \"\"\"\n",
    "    predictions = predictions.flatten()\n",
    "    actual = actual.flatten()\n",
    "    mask = mask.astype(bool).flatten()\n",
    "    \n",
    "    # Avaliação apenas em pontos válidos\n",
    "    preds_valid = predictions[mask]\n",
    "    acts_valid = actual[mask]\n",
    "    \n",
    "    if len(acts_valid) == 0:\n",
    "        print(f\"{model_name}: Nenhum ponto válido para avaliação!\")\n",
    "        return np.nan, np.nan, np.nan, model_name\n",
    "    \n",
    "    # Métricas tradicionais\n",
    "    errors = preds_valid - acts_valid\n",
    "    mse = np.square(errors).mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.abs(errors).mean()\n",
    "    nrmse = rmse / (acts_valid.max() - acts_valid.min() + 1e-12)\n",
    "    \n",
    "    # Métricas adicionais de robustez\n",
    "    median_error = np.median(np.abs(errors))\n",
    "    q75_error = np.percentile(np.abs(errors), 75)\n",
    "    \n",
    "    print(f'{model_name} (robust evaluation):')\n",
    "    print(f'MAE: {mae:.4f}')\n",
    "    print(f'RMSE: {rmse:.4f}')\n",
    "    print(f'NRMSE: {nrmse*100:.4f}%')\n",
    "    print(f'Median Error: {median_error:.4f}')\n",
    "    print(f'75th Percentile Error: {q75_error:.4f}')\n",
    "    print(f'Valid points: {len(acts_valid)}/{len(actual)} ({len(acts_valid)/len(actual)*100:.1f}%)')\n",
    "    print('')\n",
    "    \n",
    "    return rmse, mae, nrmse, model_name\n",
    "\n",
    "# ===================================================================================\n",
    "# CONFIGURAÇÕES E FUNÇÃO PRINCIPAL\n",
    "# ===================================================================================\n",
    "\n",
    "# Configurações\n",
    "BASE_DIR = \"resultados-recursive-prediction-robust\"\n",
    "PREDICTIONS_DIR = os.path.join(BASE_DIR, \"predicoes\")\n",
    "PLOTS_DIR = os.path.join(BASE_DIR, \"plots\")\n",
    "PLOTS_LOSS_DIR = os.path.join(PLOTS_DIR, \"loss\")\n",
    "PLOTS_PREDICTIONS_DIR = os.path.join(PLOTS_DIR, \"predicoes\")\n",
    "TRAINING_OUTPUT = os.path.join(BASE_DIR, 'training_output_robust.txt')\n",
    "METRICS_PATH = os.path.join(BASE_DIR, 'metrics_robust.json')\n",
    "MODEL_DIR = os.path.join(BASE_DIR, 'saved_models_robust')\n",
    "\n",
    "# Criar diretórios\n",
    "for dir_path in [BASE_DIR, PREDICTIONS_DIR, PLOTS_DIR, PLOTS_LOSS_DIR, PLOTS_PREDICTIONS_DIR, MODEL_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "LOOK_BACK = 3\n",
    "WINDOW_SIZE = 28\n",
    "THROUGHPUT_DATASETS = os.path.join('..', '..', 'datasets', 'test-recursive-lstm-test')\n",
    "\n",
    "def processo_arquivo_robusto(caminho_arquivo, arquivo):\n",
    "    \"\"\"\n",
    "    Processa um arquivo CSV com o pipeline robusto completo\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extrair nome do arquivo\n",
    "        partes = caminho_arquivo.split(os.sep)\n",
    "        if len(partes) >= 6:\n",
    "            substring_desejada = partes[4] + ' - ' + partes[5]\n",
    "        else:\n",
    "            substring_desejada = arquivo.replace('.csv', '')\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processando: {substring_desejada}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Carregar dados\n",
    "        df = pd.read_csv(caminho_arquivo, index_col='Timestamp')\n",
    "        \n",
    "        # Remover colunas desnecessárias\n",
    "        if '0' in df.columns:\n",
    "            df.drop('0', axis=1, inplace=True)\n",
    "        \n",
    "        # Criar máscara original\n",
    "        mask_total = ~(df['Throughput'].isna() | (df['Throughput'] == -1))\n",
    "        \n",
    "        # Verificar se há dados suficientes\n",
    "        if np.sum(mask_total) < 100:\n",
    "            print(f\"Dados insuficientes para {substring_desejada}\")\n",
    "            return None\n",
    "        \n",
    "        # Split treino-teste\n",
    "        train_size = int(len(df.index) * 0.8)\n",
    "        train_data = df.iloc[:train_size].copy()\n",
    "        test_data = df.iloc[train_size:].copy()\n",
    "        \n",
    "        # Máscaras\n",
    "        mask_train = mask_total.iloc[:train_size].values\n",
    "        mask_test = mask_total.iloc[train_size:].values\n",
    "        \n",
    "        print(f\"Dados de treino: {len(train_data)} pontos, {np.sum(mask_train)} válidos ({np.sum(mask_train)/len(mask_train)*100:.1f}%)\")\n",
    "        print(f\"Dados de teste: {len(test_data)} pontos, {np.sum(mask_test)} válidos ({np.sum(mask_test)/len(mask_test)*100:.1f}%)\")\n",
    "        \n",
    "        # 1. Simulação de falhas artificiais\n",
    "        print(\"\\n1. Simulando falhas artificiais no treino...\")\n",
    "        \n",
    "        regions = identify_low_missing_regions(train_data['Throughput'].values, mask_train)\n",
    "        print(f\"Regiões com poucos gaps identificadas: {len(regions)}\")\n",
    "        \n",
    "        artificial_mask, artificial_data = simulate_missing_data(\n",
    "            train_data['Throughput'].values, mask_train, \n",
    "            missing_ratio=0.15, seed=42\n",
    "        )\n",
    "        \n",
    "        original_valid = np.sum(mask_train)\n",
    "        artificial_valid = np.sum(artificial_mask)\n",
    "        print(f\"Dados válidos: {original_valid} -> {artificial_valid} (removidos: {original_valid - artificial_valid})\")\n",
    "        \n",
    "        # Aplicar interpolação nos dados com falhas artificiais\n",
    "        train_data_artificial = train_data.copy()\n",
    "        train_data_artificial['Throughput'] = artificial_data\n",
    "        train_data_artificial['Throughput'] = train_data_artificial['Throughput'].interpolate(method='linear')\n",
    "        \n",
    "        # 2. Features auxiliares\n",
    "        print(\"\\n2. Criando features auxiliares...\")\n",
    "        \n",
    "        mask_feature_train, delta_t_feature_train = create_auxiliary_features(\n",
    "            train_data_artificial['Throughput'].values, artificial_mask\n",
    "        )\n",
    "        \n",
    "        test_data_interpolated = test_data.copy()\n",
    "        test_data_interpolated['Throughput'] = test_data_interpolated['Throughput'].interpolate(method='linear')\n",
    "        \n",
    "        mask_feature_test, delta_t_feature_test = create_auxiliary_features(\n",
    "            test_data_interpolated['Throughput'].values, mask_test\n",
    "        )\n",
    "        \n",
    "        print(f\"Features auxiliares criadas: mask + delta_t\")\n",
    "        \n",
    "        # 3. Normalização e dataset com features auxiliares\n",
    "        print(\"\\n3. Preparando dados com features auxiliares...\")\n",
    "        \n",
    "        scaler = MinMaxScaler().fit(train_data_artificial[['Throughput']])\n",
    "        train_scaled = scaler.transform(train_data_artificial[['Throughput']])\n",
    "        test_scaled = scaler.transform(test_data_interpolated[['Throughput']])\n",
    "        \n",
    "        X_train, y_train, mask_y_train = create_dataset_with_auxiliary_features(\n",
    "            train_scaled.flatten(), mask_feature_train, delta_t_feature_train, look_back=LOOK_BACK\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset de treino: {X_train.shape} (3 features: valor, mask, delta_t)\")\n",
    "        \n",
    "        # 4. Grid search robusto\n",
    "        print(\"\\n4. Executando grid search robusto...\")\n",
    "        \n",
    "        best_params = grid_search_cv_robust(\n",
    "            create_robust_lstm, 64, X_train, [1e-3, 1e-4],\n",
    "            y_train, mask_y_train, [100, 200], [32, 64], [10], 'LSTM-Robust'\n",
    "        )\n",
    "        \n",
    "        # 5. Treinamento com loss ponderada\n",
    "        print(\"\\n5. Treinando modelo com loss ponderada...\")\n",
    "        \n",
    "        # CORREÇÃO: Criar modelo final com input_shape correto\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "        model = create_robust_lstm(64, input_shape, 5)\n",
    "        \n",
    "        # Pesos para treinamento\n",
    "        sample_weights = np.where(mask_y_train, 1.0, 0.3)\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=best_params['patience'],\n",
    "            restore_best_weights=True,\n",
    "            min_delta=1e-5\n",
    "        )\n",
    "        \n",
    "        # Treinar modelo final\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=best_params['epochs'],\n",
    "            batch_size=best_params['batch_size'],\n",
    "            validation_split=0.2,\n",
    "            sample_weight=sample_weights,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # 6. Predição walk-forward robusta\n",
    "        print(\"\\n6. Realizando predição walk-forward robusta...\")\n",
    "        \n",
    "        test_scaled_flat = test_scaled.flatten()\n",
    "        \n",
    "        predictions = walk_forward_validation_robust(\n",
    "            model, scaler, \n",
    "            train_scaled.flatten(),\n",
    "            test_scaled_flat,\n",
    "            mask_test,\n",
    "            mask_feature_test,\n",
    "            delta_t_feature_test,\n",
    "            look_back=LOOK_BACK,\n",
    "            window_size=WINDOW_SIZE\n",
    "        )\n",
    "        \n",
    "        # Converter predições para formato original\n",
    "        predictions = np.array(predictions).reshape(-1, 1)\n",
    "        predictions_orig = scaler.inverse_transform(predictions)\n",
    "        \n",
    "        # 7. Avaliação robusta e salvamento de resultados\n",
    "        print(\"\\n7. Avaliando e salvando resultados...\")\n",
    "        \n",
    "        actual_values = test_data['Throughput'].values[LOOK_BACK:]\n",
    "        mask_valid = mask_test[LOOK_BACK:]\n",
    "        \n",
    "        rmse, mae, nrmse, _ = validate_missing_data_prediction_robust(\n",
    "            predictions_orig, \n",
    "            actual_values.reshape(-1, 1), \n",
    "            mask_valid,\n",
    "            substring_desejada\n",
    "        )\n",
    "        \n",
    "        # Salvar métricas\n",
    "        metrics = {\n",
    "            'RMSE': float(rmse) if not np.isnan(rmse) else None,\n",
    "            'MAE': float(mae) if not np.isnan(mae) else None,\n",
    "            'NRMSE': float(nrmse) if not np.isnan(nrmse) else None,\n",
    "            'Valid_Points': f\"{np.sum(mask_valid)}/{len(mask_valid)}\",\n",
    "            'Valid_Percentage': f\"{np.sum(mask_valid)/len(mask_valid)*100:.1f}%\"\n",
    "        }\n",
    "        \n",
    "        # Salvar predições\n",
    "        pred_df = pd.DataFrame({\n",
    "            'Timestamp': test_data.index[LOOK_BACK:],\n",
    "            'Actual': actual_values,\n",
    "            'Predicted': predictions_orig.flatten(),\n",
    "            'Valid': mask_valid\n",
    "        })\n",
    "        pred_file = os.path.join(PREDICTIONS_DIR, f\"{substring_desejada}_predictions.csv\")\n",
    "        pred_df.to_csv(pred_file, index=False)\n",
    "        \n",
    "        # Salvar modelo\n",
    "        model_file = os.path.join(MODEL_DIR, f\"{substring_desejada}_model.keras\")\n",
    "        model.save(model_file)\n",
    "        \n",
    "        # Salvar histórico de treinamento\n",
    "        history_file = os.path.join(BASE_DIR, f\"{substring_desejada}_history.json\")\n",
    "        history_dict = {}\n",
    "        for key, value in history.history.items():\n",
    "            history_dict[key] = [float(v) for v in value]\n",
    "        \n",
    "        with open(history_file, 'w') as f:\n",
    "            json.dump(history_dict, f, indent=4)\n",
    "            \n",
    "        # Plotar perda de treinamento\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'Training History: {substring_desejada}')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        loss_file = os.path.join(PLOTS_LOSS_DIR, f\"{substring_desejada}_loss.png\")\n",
    "        plt.savefig(loss_file)\n",
    "        plt.close()\n",
    "        \n",
    "        # Plotar predições vs reais\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        valid_indices = np.where(mask_valid)[0]\n",
    "        plt.plot(pred_df['Timestamp'], pred_df['Actual'], 'b-', label='Actual')\n",
    "        plt.plot(pred_df['Timestamp'], pred_df['Predicted'], 'r-', alpha=0.7, label='Predicted')\n",
    "        plt.scatter(\n",
    "            pred_df.iloc[valid_indices]['Timestamp'],\n",
    "            pred_df.iloc[valid_indices]['Actual'],\n",
    "            c='green', s=15, label='Valid Points'\n",
    "        )\n",
    "        plt.title(f'Prediction vs Actual: {substring_desejada}')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel('Throughput')\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        pred_file = os.path.join(PLOTS_PREDICTIONS_DIR, f\"{substring_desejada}_prediction.png\")\n",
    "        plt.savefig(pred_file)\n",
    "        plt.close()\n",
    "        \n",
    "        # Registrar métricas no arquivo de saída\n",
    "        with open(TRAINING_OUTPUT, 'a') as f:\n",
    "            f.write(f\"\\n\\n{'='*50}\\n\")\n",
    "            f.write(f\"Resultados para: {substring_desejada}\\n\")\n",
    "            f.write(f\"RMSE: {rmse:.4f}\\n\")\n",
    "            f.write(f\"MAE: {mae:.4f}\\n\")\n",
    "            f.write(f\"NRMSE: {nrmse*100:.2f}%\\n\")\n",
    "            f.write(f\"Valid Points: {np.sum(mask_valid)}/{len(mask_valid)} ({np.sum(mask_valid)/len(mask_valid)*100:.1f}%)\\n\")\n",
    "        \n",
    "        # Atualizar métricas globais\n",
    "        global_metrics = {}\n",
    "        if os.path.exists(METRICS_PATH):\n",
    "            with open(METRICS_PATH, 'r') as f:\n",
    "                global_metrics = json.load(f)\n",
    "                \n",
    "        global_metrics[substring_desejada] = metrics\n",
    "        \n",
    "        with open(METRICS_PATH, 'w') as f:\n",
    "            json.dump(global_metrics, f, indent=4)\n",
    "            \n",
    "        print(f\"Processamento completo para {substring_desejada}\")\n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro no processamento de {arquivo}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Função principal para executar o pipeline\n",
    "    \"\"\"\n",
    "    arquivos_processados = 0\n",
    "    \n",
    "    # Inicializar arquivo de saída\n",
    "    with open(TRAINING_OUTPUT, 'w') as f:\n",
    "        f.write(\"=== INÍCIO DO TREINAMENTO ROBUSTO ===\\n\")\n",
    "    \n",
    "    # Coletar todos os arquivos CSV\n",
    "    arquivos = []\n",
    "    for root, dirs, files in os.walk(THROUGHPUT_DATASETS):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                arquivos.append((os.path.join(root, file), file))\n",
    "    \n",
    "    # Processar cada arquivo\n",
    "    for caminho, arquivo in arquivos:\n",
    "        resultado = processo_arquivo_robusto(caminho, arquivo)\n",
    "        if resultado:\n",
    "            arquivos_processados += 1\n",
    "            \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processamento completo!\")\n",
    "    print(f\"Arquivos processados: {arquivos_processados}/{len(arquivos)}\")\n",
    "    print(f\"Resultados salvos em: {BASE_DIR}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
