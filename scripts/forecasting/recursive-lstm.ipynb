{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU usage for tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 30 09:34:28 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060        Off |   00000000:01:00.0  On |                  N/A |\n",
      "| 32%   34C    P5             N/A /  115W |     440MiB /   8188MiB |     41%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2636      G   /usr/lib/xorg/Xorg                            169MiB |\n",
      "|    0   N/A  N/A      2934      G   /usr/bin/gnome-shell                           82MiB |\n",
      "|    0   N/A  N/A      3719      G   /opt/google/chrome/chrome                       2MiB |\n",
      "|    0   N/A  N/A      3770      G   ...seed-version=20250627-050030.018000         28MiB |\n",
      "|    0   N/A  N/A      4403      G   /usr/share/code/code                          125MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi # veriying if NVIDEA drive and CUDA runtime loads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 09:34:28.862976: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-30 09:34:29.041575: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751286869.116678    6221 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751286869.137550    6221 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751286869.277797    6221 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751286869.277816    6221 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751286869.277817    6221 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751286869.277818    6221 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-30 09:34:29.290117: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs detectadas: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 1) Ver todas as GPUs\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(\"GPUs detectadas:\", gpus)\n",
    "\n",
    "if gpus:\n",
    "    # 2) (Opcional) limitar a visão só à primeira GPU\n",
    "    tf.config.set_visible_devices(gpus[0], \"GPU\")\n",
    "\n",
    "    # 3) (Recomendado) liberar memória sob demanda\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x600 with 0 Axes>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LIBS\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import seaborn as sns\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "# import statsmodels.api as sm\n",
    "# from pmdarima import auto_arima\n",
    "#bibliotecas usadas no gridserach\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import keras\n",
    "\n",
    "register_matplotlib_converters()\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "plt.rc(\"figure\", figsize=(16, 6))\n",
    "plt.rc(\"font\", size=13)\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize = (16, 6), dpi = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gridsearch\n",
    "def grid_search_cv(modelo, units, X_train, learning_rates, y_train, epochs_list, batch_sizes, patiences, model_name):\n",
    "    best_loss = float('inf')\n",
    "    best_params = {}\n",
    "    for lr in learning_rates:\n",
    "        for epochs in epochs_list:\n",
    "            for batch_size in batch_sizes:\n",
    "                for patience in patiences:\n",
    "                    model = modelo(units, X_train, lr)\n",
    "                    histories = fit_model_with_cross_validation(model, X_train, y_train, model_name, patience, epochs, batch_size)\n",
    "                    mean_history = calculate_mean_history(histories)\n",
    "                    val_loss = min(mean_history['val_loss'])\n",
    "                    print(\"Val Loss: \", val_loss, \"learning rate: \", lr, \"epochs: \",  epochs, \"batch_size: \" , batch_size, \"patience: \", patience)\n",
    "                    if val_loss < best_loss:\n",
    "                        best_loss = val_loss\n",
    "                        best_params = {'learning_rate': lr, 'epochs': epochs, 'batch_size': batch_size, 'patience': patience} \n",
    "    print('O modelo '+model_name+ ' tem como melhores parametros os seguintes: learning_rate '+ str(best_params['learning_rate'])+' epochs: '+ str(best_params['epochs'])+' batch_size: '+ str(best_params['batch_size'])+ ' patience: '+ str(best_params['patience']))\n",
    "    return best_params\n",
    "\n",
    "#validação cruzada\n",
    "def fit_model_with_cross_validation(model, xtrain, ytrain, model_name, patience, epochs, batch_size):\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    fold = 1\n",
    "    histories = []\n",
    "    for train_index, val_index in tscv.split(xtrain):\n",
    "        x_train_fold, x_val_fold = xtrain[train_index], xtrain[val_index]\n",
    "        y_train_fold, y_val_fold = ytrain[train_index], ytrain[val_index]\n",
    "        early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True, min_delta=1e-5)\n",
    "        history = model.fit(x_train_fold, y_train_fold, epochs=epochs, validation_data=(x_val_fold, y_val_fold), batch_size=batch_size, callbacks=[early_stop], verbose=1)\n",
    "        print('\\n\\nTREINAMENTO - Fold', fold, 'do modelo:', model_name)\n",
    "        histories.append(history)\n",
    "        fold += 1   \n",
    "    return histories \n",
    "\n",
    "# calcula a media das metricas obtidas nos historys - validação cruzada\n",
    "def calculate_mean_history(histories):\n",
    "    mean_history = {'loss': [], 'root_mean_squared_error': [], 'val_loss': [], 'val_root_mean_squared_error': []}\n",
    "    for fold_history in histories:\n",
    "        for key in mean_history.keys():\n",
    "            mean_history[key].append(fold_history.history[key])\n",
    "    for key, values in mean_history.items():\n",
    "        max_len = max(len(val) for val in values)\n",
    "        for i in range(len(values)):\n",
    "            if len(values[i]) < max_len: #caso em que nao se treina todas as epocas (patience)\n",
    "                values[i] += [values[i][-1]] * (max_len - len(values[i])) #completa o restante da lista com o ultimo valor obtido\n",
    "    for key, values in mean_history.items():\n",
    "        mean_history[key] = [sum(vals) / len(vals) for vals in zip(*values)]\n",
    "    \n",
    "    return mean_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main functions\n",
    "\n",
    "# Create input dataset\n",
    "# The input shape should be [samples, time steps, features\n",
    "def create_dataset (X, look_back = 3):\n",
    "    Xs, ys = [], []\n",
    "    \n",
    "    for i in range(len(X)-look_back):\n",
    "        v = X[i:i+look_back]\n",
    "        Xs.append(v)\n",
    "        ys.append(X[i+look_back])\n",
    "        \n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Create LSTM model\n",
    "def create_lstm(units, train, learning_rate): \n",
    "    model = Sequential() \n",
    "    # Old Config\n",
    "    model.add(LSTM(units = units, return_sequences = True, input_shape = [train.shape[1], train.shape[2]]))\n",
    "    model.add(LSTM(units = units)) \n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=MeanSquaredError(), optimizer = Adam(learning_rate=learning_rate), metrics=[RootMeanSquaredError()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#treinamento do modelo\n",
    "def fit_model(model, xtrain, ytrain, model_name, patience, epochs, batch_size ):\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = patience, restore_best_weights=True)\n",
    "    history = model.fit(xtrain, ytrain, epochs = epochs, validation_split = 0.2, batch_size = batch_size, shuffle = True, callbacks=[early_stop]) \n",
    "    print('\\n\\nTREINAMENTO: ' + model_name)\n",
    "    return history\n",
    "\n",
    "# Make prediction\n",
    "def prediction(model, xtest, ytest, myscaler, model_name, link): \n",
    "    prediction = model.predict(xtest) \n",
    "    prediction = myscaler.inverse_transform(prediction) \n",
    "    # dataframe_prediction = pd.DataFrame(data={'Predições':prediction.flatten()})\n",
    "    dataframe_prediction = pd.DataFrame(data={'Prediction':prediction.flatten(), 'Test':ytest.flatten()})\n",
    "    #save_path = os.path.join('..', '..', 'predicoes', f'prediction {model_name} {link}.csv') \n",
    "    save_path = os.path.join('..', '..', 'results', 'bi-lstm', 'forecast', f'prediction {model_name} {link}.csv') \n",
    "    dataframe_prediction.to_csv(save_path)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "# Calculate MAE and RMSE\n",
    "def evaluate_prediction(predictions, actual, model_name):\n",
    "    errors = predictions - actual\n",
    "    mse = np.square(errors).mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "    nrmse = rmse/ ((np.max(actual))-(np.min(actual)))\n",
    "    mae = np.abs(errors).mean()\n",
    "    print(model_name + ':')\n",
    "    print('Mean Absolute Error: {:.4f}'.format(mae))\n",
    "    print('Root Mean Square Error: {:.4f}'.format(rmse))\n",
    "    print('Normalized Root Mean Square Error: {:.4f}%'.format(nrmse*100))\n",
    "    print('')\n",
    "\n",
    "    return rmse, mae, nrmse, model_name\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bits_para_megabits(df, col_vaz):\n",
    "    # Dados em MegaBits/s e fill\n",
    "    df[col_vaz] = df[col_vaz]/1000000\n",
    "    df[col_vaz] = df[col_vaz].replace(-1, df[col_vaz].mean())\n",
    "    df[col_vaz] = df[col_vaz].fillna(df[col_vaz].mean())\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "                \n",
    "def visualizacao_series(df, col_vazao, titulo):\n",
    "    df[col_vazao].plot(figsize=(18,6))\n",
    "    plt.title(titulo)\n",
    "    plt.ylabel('Vazao (Mbits/s)')\n",
    "    plt.legend() \n",
    "    plt.show()\n",
    "\n",
    "#plotar os graficos da media dos treinamentos por epocas: validação cruzada\n",
    "def plot_loss_cv(mean_history, model_name, link):\n",
    "    epochs = range(1, len(mean_history['loss']) + 1)\n",
    "    plt.plot(epochs, mean_history['loss'], label='Train Loss')\n",
    "    plt.plot(epochs, mean_history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Mean Training and Validation Loss for '+' '+link + ' '+ model_name)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_rmse_cv(mean_history):\n",
    "    epochs = range(1, len(mean_history['root_mean_squared_error']) + 1)\n",
    "    plt.plot(epochs, mean_history['root_mean_squared_error'], label='Train RMSE')\n",
    "    plt.plot(epochs, mean_history['val_root_mean_squared_error'], label='Validation RMSE')\n",
    "    plt.title('Mean Training and Validation RMSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "########################################### plote dos graficos de treinamento ###################################################################################\n",
    " #Plot train loss and validation loss\n",
    "def plot_loss(history, model_name, link):\n",
    "     plt.figure(figsize = (15, 6), dpi=100)\n",
    "     plt.plot(history.history['loss'])\n",
    "     plt.plot(history.history['val_loss'])\n",
    "     plt.title('Model Train vs Validation Loss for '+' '+link + ' '+ model_name)\n",
    "     plt.ylabel('Loss')\n",
    "     plt.xlabel('Epoch')\n",
    "     plt.legend(['Train loss', 'Validation loss'], loc='upper right')\n",
    "def plot_rmse(history, model_name, link):\n",
    "     plt.figure(figsize = (15, 6), dpi=100)\n",
    "     plt.plot(history.history['rmse'])\n",
    "     plt.plot(history.history['val_rmse'])\n",
    "     plt.title('Model Train vs RMSE for '+' '+link + ' '+ model_name)\n",
    "     plt.ylabel('rmse')\n",
    "     plt.xlabel('Epoch')\n",
    "     plt.legend(['Train rmse', 'Validation loss'], loc='upper right')\n",
    "################################################################################################################################################################\n",
    " \n",
    "\n",
    "def plot_future(predictionLSTM, y_test, link):\n",
    "    plt.figure(figsize=(15, 6), dpi=100)\n",
    "    range_future = len(y_test)\n",
    "    plt.plot(np.arange(range_future), np.array(y_test), label='Test data')\n",
    "    plt.plot(np.arange(range_future), np.array(predictionLSTM), label='LSTM')\n",
    "    # dict_to_dataframe_prediction = {\n",
    "    #     # \"range_future\": np.arange(range_future),\n",
    "    #     f\"prediction{model_name}\": np.array(prediction.squeeze())\n",
    "    # }\n",
    "    \n",
    "    plt.title('Test data vs prediction for '+ link)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Mbis/s')\n",
    "    save_path = os.path.join('..', '..', 'results', 'bi-lstm', 'plots', link + '.png')\n",
    "    save_path = os.path.normpath(save_path)  \n",
    "\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    try:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"A figura foi salva com sucesso em: {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar a figura: {e}\")\n",
    "    plt.show()\n",
    "\n",
    "    # #Tenta salvar a fig\n",
    "    # save_path = os.path.join('..', '..', 'graficos', 'predicoes', 'round_2', 'graficos', link + '.png')\n",
    "\n",
    "    # #save_path = '../../graficos/predicoes/round_2/graficos/' + link + '.png'\n",
    "    # try:\n",
    "    #     plt.savefig(save_path)\n",
    "    #     print(f\"A figura foi salva com sucesso em: {save_path}\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Erro ao salvar a figura: {e}\")\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "# Plot test data vs prediction\n",
    "# def plot_future(predictionGRU, predictionLSTM, y_test, link):\n",
    "#     plt.figure(figsize=(15, 6), dpi=100)\n",
    "#     range_future = len(y_test)\n",
    "#     plt.plot(np.arange(range_future), np.array(y_test), label='Test data')\n",
    "#     plt.plot(np.arange(range_future), np.array(predictionGRU), label='GRU')\n",
    "#     plt.plot(np.arange(range_future), np.array(predictionLSTM), label='LSTM')\n",
    "#     # dict_to_dataframe_prediction = {\n",
    "#     #     # \"range_future\": np.arange(range_future),\n",
    "#     #     f\"prediction{model_name}\": np.array(prediction.squeeze())\n",
    "#     # }\n",
    "    \n",
    "#     plt.title('Test data vs prediction for '+ link)\n",
    "#     plt.legend(loc='upper left')\n",
    "#     plt.xlabel('Time')\n",
    "#     plt.ylabel('Mbis/s')\n",
    "\n",
    "#     #Tenta salvar a fig\n",
    "#     save_path = '../../graficos/predicoes/round_2/graficos/' + link + '.png'\n",
    "#     try:\n",
    "#         plt.savefig(save_path)\n",
    "#         print(f\"A figura foi salva com sucesso em: {save_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erro ao salvar a figura: {e}\")\n",
    "\n",
    "#     plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "TRAINING_OUTPUT = os.path.join('training_output.txt')\n",
    "THROUGHPUT_DATASETS = os.path.join('..', '..', 'datasets', 'test-recursive-lstm')\n",
    "MODEL = os.path.join(\"..\", \"..\", 'modelo_salvo')\n",
    "METRICS = os.path.join('..', '..', 'results', 'bi-lstm', 'evaluation_rmse_mae_2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para salvar o modelo\n",
    "def save_model(model, directory, substring_desejada, modelo):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    file_path = os.path.join(directory, f'{substring_desejada +modelo} - final_model.keras')\n",
    "    model.save(file_path)\n",
    "    print(f\"Modelo salvo como '{file_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main\n",
    "#### Model training and predcition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Redirecionar saída padrão para um arquivo com codificação utf-8\n",
    "# orig_stdout = sys.stdout\n",
    "# f = open(TRAINING_OUTPUT, 'w', encoding='utf-8')\n",
    "# sys.stdout = f\n",
    "\n",
    "# #treinamento e predição\n",
    "# # Defina o diretório raiz onde deseja iniciar a busca\n",
    "# diretorio_raiz = TRAINING_OUTPUT\n",
    "\n",
    "# evaluation = {}\n",
    "\n",
    "# # Itere pelos diretórios e subdiretórios\n",
    "# for pasta_raiz, subpastas, arquivos in os.walk(diretorio_raiz):\n",
    "#     for arquivo in arquivos:\n",
    "#         if arquivo.endswith('.csv'):\n",
    "#             # Construa o caminho completo para o arquivo\n",
    "#             caminho_arquivo = os.path.join(pasta_raiz, arquivo)\n",
    "\n",
    "#         try:\n",
    "#                 # Título parser\n",
    "#                 # partes = caminho_arquivo.split(\"\\\\\")\n",
    "#                 # if len(partes) >= 2:\n",
    "#                 # #     substring_desejada = \"/\".join(partes[1:])  # Acesse as partes a partir da segunda e as una com barras invertidas\n",
    "#                 partes = caminho_arquivo.split(os.sep)\n",
    "#                 #partes = caminho_arquivo.split(\"/\")\n",
    "#                 substring_desejada = partes[4] + ' - ' + partes[5]  # Acesse as partes a partir da segunda e as una com barras invertidas\n",
    "\n",
    "#                 df = pd.read_csv(caminho_arquivo, index_col='Timestamp')\n",
    "#                 if '0' in df.columns:\n",
    "#                     df = df.drop('0', axis=1)\n",
    "                \n",
    "#                 # Regularizar o dataset para megabits/s\n",
    "#                 bits_para_megabits(df, 'Throughput')\n",
    "\n",
    "#                 # Visualização das séries\n",
    "#                 # visualizacao_series(df, 'Vazao', substring_desejada)\n",
    "\n",
    "#                 print('###################### '+substring_desejada+' ##########################')\n",
    "\n",
    "#                 #####################################################\n",
    "#                 ########                   FORECASTING            ###\n",
    "#                 #####################################################\n",
    "\n",
    "#                 tf.random.set_seed(7)\n",
    "                \n",
    "#                 # *********** acho que vou ter que fazer uma máscara de valores faltantes **********\n",
    "#                 # Split train data and test data\n",
    "#                 tamanho = int(len(df.index) * 0.8) \n",
    "#                 train_size = tamanho\n",
    "\n",
    "#                 # train_data = df.WC.loc[:train_size] -----> it gives a series\n",
    "#                 # Do not forget use iloc to select a number of rows\n",
    "#                 train_data = df[:train_size]\n",
    "#                 test_data = df[train_size:]\n",
    "                \n",
    "#                 # Criar uma instância do MinMaxScaler\n",
    "#                 train_data = train_data['Throughput'].values.reshape(-1, 1) # reshape serve para deixar claro que cada timestamp tem uma variável observada (throughput)\n",
    "#                 test_data = test_data['Throughput'].values.reshape(-1, 1)\n",
    "\n",
    "#                 scaler = MinMaxScaler().fit(train_data)\n",
    "\n",
    "#                 train_scaled = scaler.transform(train_data)\n",
    "#                 test_scaled = scaler.transform(test_data)\n",
    "\n",
    "#                 # ******* acho que aqui vou adicionar um loop para mover a janela ************\n",
    "#                 X_train, y_train = create_dataset(train_scaled)\n",
    "#                 X_test, y_test = create_dataset(test_scaled)\n",
    "\n",
    "#                 #grid search para encontrar os melhores parametros\n",
    "#                 list_lr = [0.00001] # learning rate, não muda\n",
    "#                 list_epochs = [100, 300, 500] # épocas\n",
    "#                 list_bs = [32, 64, 128] # bash (lotes)\n",
    "#                 list_pat = [5] # patiente para early stopping\n",
    "\n",
    "#                 caminho_modelo_salvo = MODEL\n",
    "#                 best_params_lstm = grid_search_cv(create_lstm, 64, X_train, list_lr, y_train, list_epochs, list_bs, list_pat, 'lstm')\n",
    "#                 #tendo os melhores parametros, eu crio o modelo e treino ele com os melhores\n",
    "#                 model_lstm = create_lstm(64, X_train, best_params_lstm['learning_rate'])\n",
    "\n",
    "#                 prev_history_lstm = fit_model_with_cross_validation(model_lstm, X_train, y_train, 'lstm', best_params_lstm['patience'], best_params_lstm['epochs'], best_params_lstm['batch_size'])\n",
    "#                 history_lstm = calculate_mean_history(prev_history_lstm)\n",
    "                \n",
    "#                 #salvando o modelo\n",
    "#                 save_model(model_lstm, caminho_modelo_salvo, substring_desejada, 'LSTM')\n",
    "\n",
    "#                 # Transform data back to original data space\n",
    "#                 y_test = scaler.inverse_transform(y_test)\n",
    "#                 y_train = scaler.inverse_transform(y_train)\n",
    "\n",
    "#                 prediction_lstm = prediction(model_lstm, X_test, y_test, scaler, 'LSTM', link=substring_desejada)\n",
    "                \n",
    "#                 plot_loss_cv(history_lstm, 'LSTM', substring_desejada)\n",
    "                \n",
    "#                 plot_future(prediction_lstm, y_test, link = substring_desejada)\n",
    "#                 # lstm_forecast = plot_future(prediction_lstm, 'LSTM', y_test, link = substring_desejada)\n",
    "\n",
    "#                 # plot_future_comparing_train_test (X_train, prediction_gru[1], 'GRU', substring_desejada)\n",
    "#                 # plot_future_comparing_train_test (X_train, prediction_lstm[1], 'LSTM', substring_desejada)\n",
    "#                 lstm_evaluation = evaluate_prediction(prediction_lstm, y_test, 'LSTM')\n",
    "\n",
    "#                 chave_lstm = f\"{substring_desejada}, {lstm_evaluation[3]}\"  # Crie uma chave baseada no valor de i\n",
    "#                 tupla = (lstm_evaluation[0], lstm_evaluation[1], lstm_evaluation[2])  # Crie uma tupla com valores baseados em i\n",
    "#                 evaluation[chave_lstm] = tupla  # Adicione a chave e a tupla ao dicionário\n",
    "\n",
    "#                 # print(evaluation+'\\n\\n')\n",
    "                    \n",
    "#         except pd.errors.EmptyDataError:\n",
    "#             # Arquivo vazio\n",
    "#             print(f\"Arquivo: {arquivo} - O arquivo está vazio, subpasta: {pasta_raiz}\")\n",
    "#         except Exception as e:\n",
    "#             # Tratar outras exceções, se necessário\n",
    "#                 print(f\"Arquivo: {arquivo}, subpasta: {pasta_raiz} - Erro: {str(e)}\")\n",
    "\n",
    "    \n",
    "#         novo_dicionario = {}\n",
    "#         import json\n",
    "        \n",
    "#         # Itere pelo dicionário original\n",
    "#         for chave, valores in evaluation.items():\n",
    "#             valor1, valor2, valor3 = valores  # Desempacote os valores da tupla\n",
    "#             novo_dicionario[chave] = {'RMSE': valor1, 'MAE': valor2, 'NRMSE': valor3}\n",
    "\n",
    "#         json_path = METRICS\n",
    "#         with open(json_path, 'w') as arquivo:\n",
    "#             json.dump(novo_dicionario, arquivo, indent=4)\n",
    "\n",
    "#         # with open('../../graficos/predicoes/round_2/evaluation_rmse_mae_2.json', 'w') as arquivo:\n",
    "#         #     json.dump(novo_dicionario, arquivo, indent=4)\n",
    "\n",
    "#     # with open('..\\\\..\\\\graficos\\\\predicoes\\\\round_2\\\\evaluation_rmse_mae_2.json', 'w') as arquivo:\n",
    "#     #     json.dump(novo_dicionario, arquivo, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clara/Documentos/UECE-RNP-2024/.venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Redirecionar saída padrão para um arquivo com codificação utf-8\n",
    "orig_stdout = sys.stdout\n",
    "f = open(TRAINING_OUTPUT, 'w', encoding='utf-8')\n",
    "sys.stdout = f\n",
    "\n",
    "#treinamento e predição\n",
    "# Defina o diretório raiz onde deseja iniciar a busca\n",
    "diretorio_raiz = THROUGHPUT_DATASETS\n",
    "\n",
    "evaluation = {}\n",
    "\n",
    "# Itere pelos diretórios e subdiretórios\n",
    "for pasta_raiz, subpastas, arquivos in os.walk(diretorio_raiz):\n",
    "    for arquivo in arquivos:\n",
    "        if arquivo.endswith('.csv'):\n",
    "            # Construa o caminho completo para o arquivo\n",
    "            caminho_arquivo = os.path.join(pasta_raiz, arquivo)\n",
    "\n",
    "        try:\n",
    "                # Título parser\n",
    "                # partes = caminho_arquivo.split(\"\\\\\")\n",
    "                # if len(partes) >= 2:\n",
    "                # #     substring_desejada = \"/\".join(partes[1:])  # Acesse as partes a partir da segunda e as una com barras invertidas\n",
    "                partes = caminho_arquivo.split(os.sep)\n",
    "                #partes = caminho_arquivo.split(\"/\")\n",
    "                substring_desejada = partes[4] + ' - ' + partes[5]  # Acesse as partes a partir da segunda e as una com barras invertidas\n",
    "\n",
    "                df = pd.read_csv(caminho_arquivo, index_col='Timestamp')\n",
    "                if '0' in df.columns:\n",
    "                    df = df.drop('0', axis=1)\n",
    "                \n",
    "                # Regularizar o dataset para megabits/s\n",
    "                bits_para_megabits(df, 'Throughput')\n",
    "\n",
    "                # Visualização das séries\n",
    "                # visualizacao_series(df, 'Vazao', substring_desejada)\n",
    "\n",
    "                print('###################### '+substring_desejada+' ##########################')\n",
    "\n",
    "                #####################################################\n",
    "                ########                   FORECASTING            ###\n",
    "                #####################################################\n",
    "\n",
    "                tf.random.set_seed(7)\n",
    "                \n",
    "                # *********** acho que vou ter que fazer uma máscara de valores faltantes **********\n",
    "                # Split train data and test data\n",
    "                tamanho = int(len(df.index) * 0.8) \n",
    "                train_size = tamanho\n",
    "\n",
    "                # train_data = df.WC.loc[:train_size] -----> it gives a series\n",
    "                # Do not forget use iloc to select a number of rows\n",
    "                train_data = df[:train_size]\n",
    "                test_data = df[train_size:]\n",
    "                \n",
    "                # Criar uma instância do MinMaxScaler\n",
    "                train_data = train_data['Throughput'].values.reshape(-1, 1) # reshape serve para deixar claro que cada timestamp tem uma variável observada (throughput)\n",
    "                test_data = test_data['Throughput'].values.reshape(-1, 1)\n",
    "\n",
    "                scaler = MinMaxScaler().fit(train_data)\n",
    "\n",
    "                train_scaled = scaler.transform(train_data)\n",
    "                test_scaled = scaler.transform(test_data)\n",
    "\n",
    "                # Preparar dados iniciais para treinamento\n",
    "                X_train_original, y_train_original = create_dataset(train_scaled)\n",
    "\n",
    "                #grid search para encontrar os melhores parametros (apenas uma vez)\n",
    "                list_lr = [0.00001] # learning rate, não muda\n",
    "                list_epochs = [100, 300, 500] # épocas\n",
    "                list_bs = [32, 64, 128] # bash (lotes)\n",
    "                list_pat = [5] # patiente para early stopping\n",
    "\n",
    "                # teste de funcionamento\n",
    "                # list_lr = [0.00001] # learning rate, não muda\n",
    "                # list_epochs = [100] # épocas\n",
    "                # list_bs = [32] # bash (lotes)\n",
    "                # list_pat = [5] # patiente para early stopping\n",
    "\n",
    "                caminho_modelo_salvo = MODEL\n",
    "                best_params_lstm = grid_search_cv(create_lstm, 64, X_train_original, list_lr, y_train_original, list_epochs, list_bs, list_pat, 'lstm')\n",
    "\n",
    "                # ******* IMPLEMENTAÇÃO DA JANELA DESLIZANTE ************\n",
    "                janela_tamanho = 20  # Tamanho da janela ajustável\n",
    "                num_clusters = len(test_scaled) // janela_tamanho\n",
    "                \n",
    "                # Lista para armazenar todas as predições\n",
    "                todas_predicoes = []\n",
    "                \n",
    "                print(f\"Processando {num_clusters} clusters de tamanho {janela_tamanho}\")\n",
    "                \n",
    "                for cluster_idx in range(num_clusters):\n",
    "                    print(f\"Processando cluster {cluster_idx + 1}/{num_clusters}\")\n",
    "                    \n",
    "                    # Definir início e fim do cluster atual\n",
    "                    inicio_cluster = cluster_idx * janela_tamanho\n",
    "                    fim_cluster = inicio_cluster + janela_tamanho\n",
    "                    \n",
    "                    # Extrair o cluster atual dos dados de teste\n",
    "                    cluster_atual = test_scaled[inicio_cluster:fim_cluster]\n",
    "                    \n",
    "                    # Criar novo conjunto de treinamento: treino_original[cluster_idx*janela_tamanho:] + clusters anteriores\n",
    "                    if cluster_idx == 0:\n",
    "                        # Primeiro cluster: usar apenas dados de treino originais\n",
    "                        novo_train_data = train_scaled\n",
    "                    else:\n",
    "                        # Clusters subsequentes: treino_original[inicio_cluster:] + clusters anteriores\n",
    "                        train_ajustado = train_scaled[inicio_cluster:]\n",
    "                        clusters_anteriores = test_scaled[:inicio_cluster]\n",
    "                        novo_train_data = np.vstack([train_ajustado, clusters_anteriores])\n",
    "                    \n",
    "                    # Criar datasets para o modelo\n",
    "                    X_train_novo, y_train_novo = create_dataset(novo_train_data)\n",
    "                    \n",
    "                    # Criar e treinar novo modelo para este cluster\n",
    "                    # model_lstm_cluster = create_lstm(64, X_train_novo, best_params_lstm['learning_rate'])\n",
    "                    best_params_lstm = grid_search_cv(create_lstm, 64, X_train_original, list_lr, y_train_original, list_epochs, list_bs, list_pat, 'lstm')\n",
    "                    model_lstm_cluster = create_lstm(64, X_train_novo, best_params_lstm['learning_rate'])\n",
    "                    # Treinar o modelo com os melhores parâmetros encontrados\n",
    "                    prev_history_lstm = fit_model_with_cross_validation(\n",
    "                        best_params_lstm, X_train_novo, y_train_novo, 'lstm', \n",
    "                        best_params_lstm['patience'], best_params_lstm['epochs'], best_params_lstm['batch_size']\n",
    "                    )\n",
    "                    \n",
    "                    # Preparar dados do cluster para predição\n",
    "                    X_cluster, y_cluster = create_dataset(cluster_atual)\n",
    "                    \n",
    "                    # Fazer predição para o cluster atual\n",
    "                    if len(X_cluster) > 0:  # Verificar se há dados suficientes para predição\n",
    "                        predicao_cluster = model_lstm_cluster.predict(X_cluster)\n",
    "                        predicao_cluster = scaler.inverse_transform(predicao_cluster)\n",
    "                        todas_predicoes.append(predicao_cluster)\n",
    "                        \n",
    "                        print(f\"Cluster {cluster_idx + 1}: {len(predicao_cluster)} predições geradas\")\n",
    "                    else:\n",
    "                        print(f\"Cluster {cluster_idx + 1}: Dados insuficientes para predição\")\n",
    "                \n",
    "                # Concatenar todas as predições\n",
    "                if todas_predicoes:\n",
    "                    prediction_lstm_final = np.vstack(todas_predicoes)\n",
    "                    \n",
    "                    # Preparar dados de teste para avaliação (apenas a parte que foi predita)\n",
    "                    test_data_avaliacao = test_scaled[:len(prediction_lstm_final)]\n",
    "                    X_test_final, y_test_final = create_dataset(test_data_avaliacao)\n",
    "                    y_test_final = scaler.inverse_transform(y_test_final)\n",
    "                    \n",
    "                    # Ajustar tamanhos se necessário\n",
    "                    min_len = min(len(prediction_lstm_final), len(y_test_final))\n",
    "                    prediction_lstm_final = prediction_lstm_final[:min_len]\n",
    "                    y_test_final = y_test_final[:min_len]\n",
    "                    \n",
    "                    print(f\"Predição final: {len(prediction_lstm_final)} pontos\")\n",
    "                    \n",
    "                    # Salvar o último modelo treinado\n",
    "                    save_model(model_lstm_cluster, caminho_modelo_salvo, substring_desejada, 'LSTM')\n",
    "                    \n",
    "                    # Transform data back to original data space para visualização\n",
    "                    y_train_viz = scaler.inverse_transform(y_train_original)\n",
    "                    \n",
    "                    # Plotar resultados usando a última história de treinamento\n",
    "                    history_lstm = calculate_mean_history(prev_history_lstm)\n",
    "                    plot_loss_cv(history_lstm, 'LSTM', substring_desejada)\n",
    "                    \n",
    "                    # Criar tupla compatível com plot_future (predição, dados_treino)\n",
    "                    prediction_tuple = (prediction_lstm_final, y_train_viz)\n",
    "                    # plot_future(prediction_tuple, y_test_final, link=substring_desejada)\n",
    "                    \n",
    "                    # # Avaliar predição final\n",
    "                    # lstm_evaluation = evaluate_prediction(prediction_tuple, y_test_final, 'LSTM')\n",
    "\n",
    "                    plot_future(prediction_lstm_final, y_test_final, link=substring_desejada)\n",
    "                    lstm_evaluation = evaluate_prediction(prediction_lstm_final, y_test_final, 'LSTM')\n",
    "                    \n",
    "                    chave_lstm = f\"{substring_desejada}, {lstm_evaluation[3]}\"\n",
    "                    tupla = (lstm_evaluation[0], lstm_evaluation[1], lstm_evaluation[2])\n",
    "                    evaluation[chave_lstm] = tupla\n",
    "                    \n",
    "                    print(f\"Avaliação final - RMSE: {lstm_evaluation[0]:.4f}, MAE: {lstm_evaluation[1]:.4f}, NRMSE: {lstm_evaluation[2]:.4f}\")\n",
    "                    \n",
    "                else:\n",
    "                    print(\"Nenhuma predição foi gerada\")\n",
    "                    \n",
    "        except pd.errors.EmptyDataError:\n",
    "            # Arquivo vazio\n",
    "            print(f\"Arquivo: {arquivo} - O arquivo está vazio, subpasta: {pasta_raiz}\")\n",
    "        except Exception as e:\n",
    "            # Tratar outras exceções, se necessário\n",
    "                print(f\"Arquivo: {arquivo}, subpasta: {pasta_raiz} - Erro: {str(e)}\")\n",
    "\n",
    "    \n",
    "        novo_dicionario = {}\n",
    "        import json\n",
    "        \n",
    "        # Itere pelo dicionário original\n",
    "        for chave, valores in evaluation.items():\n",
    "            valor1, valor2, valor3 = valores  # Desempacote os valores da tupla\n",
    "            novo_dicionario[chave] = {'RMSE': valor1, 'MAE': valor2, 'NRMSE': valor3}\n",
    "\n",
    "        json_path = METRICS\n",
    "        with open(json_path, 'w') as arquivo:\n",
    "            json.dump(novo_dicionario, arquivo, indent=4)\n",
    "\n",
    "        # with open('../../graficos/predicoes/round_2/evaluation_rmse_mae_2.json', 'w') as arquivo:\n",
    "        #     json.dump(novo_dicionario, arquivo, indent=4)\n",
    "\n",
    "    # with open('..\\\\..\\\\graficos\\\\predicoes\\\\round_2\\\\evaluation_rmse_mae_2.json', 'w') as arquivo:\n",
    "    #     json.dump(novo_dicionario, arquivo, indent=4)\n",
    "with open(METRICS, 'w', encoding='utf-8') as fp:\n",
    "    json.dump(\n",
    "        {k: {'RMSE': v[0], 'MAE': v[1], 'NRMSE': v[2]} for k, v in evaluation.items()},\n",
    "        fp, indent=4\n",
    "    )\n",
    "print(f\"Arquivo de métricas salvo em {METRICS}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
