{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221fbf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "import tensorflow.keras as keras\n",
    "import json\n",
    "\n",
    "# ===================================================================================\n",
    "# FUNÇÕES AUXILIARES E UTILITÁRIAS\n",
    "# ===================================================================================\n",
    "\n",
    "def bits_para_megabits(df, col_vaz):\n",
    "    \"\"\"Converte bits para megabits e trata valores faltantes\"\"\"\n",
    "    df[col_vaz] = df[col_vaz] / 1000000\n",
    "    df[col_vaz] = df[col_vaz].replace(-1, df[col_vaz].mean())\n",
    "    df[col_vaz] = df[col_vaz].fillna(df[col_vaz].mean())\n",
    "    return df\n",
    "\n",
    "def linear_interpolation(df, limit_direction='both', method='linear'):\n",
    "    \"\"\"Interpolação linear para dados faltantes\"\"\"\n",
    "    df_imputed = df.interpolate(method=method, limit_direction=limit_direction)\n",
    "    df['Throughput'] = df['Throughput'].fillna(df_imputed['Throughput'])\n",
    "    return df\n",
    "\n",
    "def create_dataset(X, look_back=3):\n",
    "    \"\"\"Cria dataset com janelas temporais para LSTM\"\"\"\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - look_back):\n",
    "        v = X[i:i + look_back]\n",
    "        Xs.append(v)\n",
    "        ys.append(X[i + look_back])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "def create_lstm(units, train, learning_rate):\n",
    "    \"\"\"Cria modelo LSTM com configuração otimizada\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=units, return_sequences=True, input_shape=[train.shape[1], train.shape[2]]))\n",
    "    model.add(LSTM(units=units))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(\n",
    "        loss=Huber(delta=0.25),\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        metrics=[RootMeanSquaredError()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# ===================================================================================\n",
    "# FUNÇÃO PRINCIPAL: WALK-FORWARD COM TRAINING DECAY\n",
    "# ===================================================================================\n",
    "\n",
    "def walk_forward_validation_with_decay(model, scaler, train_data, test_data, mask_test, \n",
    "                                      look_back, window_size, decay_factor=0.95, \n",
    "                                      confidence_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Executa walk-forward validation com training decay para reduzir propagação de erros.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - model: Modelo LSTM pré-treinado\n",
    "    - scaler: Scaler para normalização\n",
    "    - train_data: Dados de treino normalizados\n",
    "    - test_data: Dados de teste normalizados\n",
    "    - mask_test: Máscara indicando dados reais (True) vs ausentes (False)\n",
    "    - look_back: Tamanho da janela histórica\n",
    "    - window_size: Tamanho da janela de predição\n",
    "    - decay_factor: Fator de decaimento para dados imputados (0.95 = 5% de decaimento)\n",
    "    - confidence_threshold: Limiar de confiança para usar predições\n",
    "    \n",
    "    Retorna:\n",
    "    - predictions: Lista de predições em escala original\n",
    "    - confidence_scores: Scores de confiança para cada predição\n",
    "    \"\"\"\n",
    "    \n",
    "    # Estado inicial: últimos pontos do treino (todos são dados reais)\n",
    "    state = train_data[-look_back:].reshape(1, look_back, 1)\n",
    "    predictions = []\n",
    "    confidence_scores = []\n",
    "    \n",
    "    # Pesos iniciais: todos os pontos do estado inicial têm peso máximo (dados reais)\n",
    "    state_weights = np.ones(look_back) ## ****isso é um problema. eles nao tem peso maximo pq alguns sao impoutados\n",
    "    \n",
    "    print(f\"Iniciando walk-forward validation com decay_factor={decay_factor}\")\n",
    "    \n",
    "    for i in range(0, len(test_data), window_size):\n",
    "        end_idx = min(i + window_size, len(test_data))\n",
    "        window_data = test_data[i:end_idx]\n",
    "        window_mask = mask_test[i:end_idx]\n",
    "        \n",
    "        window_preds = []\n",
    "        window_confidences = []\n",
    "        current_state = state.copy()\n",
    "        current_weights = state_weights.copy()\n",
    "        \n",
    "        print(f\"Processando janela {i//window_size + 1}: índices {i} a {end_idx-1}\")\n",
    "        \n",
    "        # Predição passo-a-passo dentro da janela\n",
    "        for j in range(len(window_data)):\n",
    "            # Calcula confiança baseada nos pesos do estado atual\n",
    "            avg_weight = np.mean(current_weights)\n",
    "            confidence = min(avg_weight, 1.0)\n",
    "            \n",
    "            # Faz predição com estado atual\n",
    "            pred_scaled = model.predict(current_state, verbose=0)\n",
    "            pred_orig = scaler.inverse_transform(pred_scaled)[0, 0]\n",
    "            \n",
    "            window_preds.append(pred_orig)\n",
    "            window_confidences.append(confidence)\n",
    "            \n",
    "            # Determina o próximo ponto para o estado\n",
    "            if window_mask[j]:  # Dado real disponível\n",
    "                new_point = window_data[j]\n",
    "                new_weight = 1.0  # Peso máximo para dados reais\n",
    "                print(f\"  Passo {j+1}: Usando dado real (peso=1.0, confiança={confidence:.3f})\")\n",
    "            else:  # Usar predição\n",
    "                new_point = pred_scaled[0, 0]\n",
    "                # Peso da predição é baseado na confiança atual e no decay\n",
    "                new_weight = confidence * decay_factor\n",
    "                print(f\"  Passo {j+1}: Usando predição (peso={new_weight:.3f}, confiança={confidence:.3f})\")\n",
    "            \n",
    "            # Atualiza estado: remove ponto mais antigo, adiciona novo\n",
    "            current_state = np.roll(current_state, -1, axis=1)\n",
    "            current_state[0, -1, 0] = new_point\n",
    "            \n",
    "            # Atualiza pesos: remove peso mais antigo, adiciona novo\n",
    "            current_weights = np.roll(current_weights, -1)\n",
    "            current_weights[-1] = new_weight\n",
    "            \n",
    "            # Aplica decay aos pesos dos dados imputados anteriores\n",
    "            for k in range(len(current_weights) - 1):\n",
    "                if current_weights[k] < 1.0:  # Apenas para dados imputados\n",
    "                    current_weights[k] *= decay_factor\n",
    "        \n",
    "        predictions.extend(window_preds)\n",
    "        confidence_scores.extend(window_confidences)\n",
    "        \n",
    "        # Mantém estado para próxima janela\n",
    "        state = current_state\n",
    "        state_weights = current_weights\n",
    "        \n",
    "        print(f\"  Confiança média da janela: {np.mean(window_confidences):.3f}\")\n",
    "    \n",
    "    return predictions, confidence_scores\n",
    "\n",
    "# ===================================================================================\n",
    "# VALIDAÇÃO CRUZADA COM GRID SEARCH\n",
    "# ===================================================================================\n",
    "\n",
    "def fit_model_with_cross_validation(model, xtrain, ytrain, model_name, patience, epochs, batch_size):\n",
    "    \"\"\"Validação cruzada temporal\"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    fold = 1\n",
    "    histories = []\n",
    "    \n",
    "    for train_index, val_index in tscv.split(xtrain):\n",
    "        x_train_fold, x_val_fold = xtrain[train_index], xtrain[val_index]\n",
    "        y_train_fold, y_val_fold = ytrain[train_index], ytrain[val_index]\n",
    "        \n",
    "        early_stop = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=patience, \n",
    "            restore_best_weights=True, \n",
    "            min_delta=1e-5\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            x_train_fold, y_train_fold, \n",
    "            epochs=epochs, \n",
    "            validation_data=(x_val_fold, y_val_fold), \n",
    "            batch_size=batch_size, \n",
    "            callbacks=[early_stop], \n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(f'\\n\\nTREINAMENTO - Fold {fold} do modelo: {model_name}')\n",
    "        histories.append(history)\n",
    "        fold += 1\n",
    "    \n",
    "    return histories\n",
    "\n",
    "def calculate_mean_history(histories):\n",
    "    \"\"\"Calcula média das métricas de validação cruzada\"\"\"\n",
    "    mean_history = {\n",
    "        'loss': [], \n",
    "        'root_mean_squared_error': [], \n",
    "        'val_loss': [], \n",
    "        'val_root_mean_squared_error': []\n",
    "    }\n",
    "    \n",
    "    for fold_history in histories:\n",
    "        for key in mean_history.keys():\n",
    "            mean_history[key].append(fold_history.history[key])\n",
    "    \n",
    "    # Normaliza comprimentos (caso early stopping pare em épocas diferentes)\n",
    "    for key, values in mean_history.items():\n",
    "        max_len = max(len(val) for val in values)\n",
    "        for i in range(len(values)):\n",
    "            if len(values[i]) < max_len:\n",
    "                values[i] += [values[i][-1]] * (max_len - len(values[i]))\n",
    "    \n",
    "    # Calcula médias\n",
    "    for key, values in mean_history.items():\n",
    "        mean_history[key] = [sum(vals) / len(vals) for vals in zip(*values)]\n",
    "    \n",
    "    return mean_history\n",
    "\n",
    "def grid_search_cv(modelo, units, X_train, learning_rates, y_train, epochs_list, \n",
    "                  batch_sizes, patiences, model_name):\n",
    "    \"\"\"Grid search com validação cruzada\"\"\"\n",
    "    best_loss = float('inf')\n",
    "    best_params = {}\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        for epochs in epochs_list:\n",
    "            for batch_size in batch_sizes:\n",
    "                for patience in patiences:\n",
    "                    model = modelo(units, X_train, lr)\n",
    "                    histories = fit_model_with_cross_validation(\n",
    "                        model, X_train, y_train, model_name, patience, epochs, batch_size\n",
    "                    )\n",
    "                    \n",
    "                    mean_history = calculate_mean_history(histories)\n",
    "                    val_loss = min(mean_history['val_loss'])\n",
    "                    \n",
    "                    print(f\"Val Loss: {val_loss:.6f}, LR: {lr}, Epochs: {epochs}, \"\n",
    "                          f\"Batch: {batch_size}, Patience: {patience}\")\n",
    "                    \n",
    "                    if val_loss < best_loss:\n",
    "                        best_loss = val_loss\n",
    "                        best_params = {\n",
    "                            'learning_rate': lr, \n",
    "                            'epochs': epochs, \n",
    "                            'batch_size': batch_size, \n",
    "                            'patience': patience\n",
    "                        }\n",
    "    \n",
    "    print(f'\\nMelhores parâmetros para {model_name}:')\n",
    "    print(f\"Learning Rate: {best_params['learning_rate']}\")\n",
    "    print(f\"Epochs: {best_params['epochs']}\")\n",
    "    print(f\"Batch Size: {best_params['batch_size']}\")\n",
    "    print(f\"Patience: {best_params['patience']}\")\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "# ===================================================================================\n",
    "# AVALIAÇÃO COM DADOS FALTANTES\n",
    "# ===================================================================================\n",
    "\n",
    "def validate_missing_data_prediction(predictions, actual, mask, model_name):\n",
    "    \"\"\"Avalia métricas apenas em pontos com dados reais\"\"\"\n",
    "    predictions = np.array(predictions).flatten()\n",
    "    actual = actual.flatten()\n",
    "    mask = mask.astype(bool).flatten()\n",
    "    \n",
    "    # Seleciona apenas pontos válidos\n",
    "    preds_valid = predictions[mask]\n",
    "    acts_valid = actual[mask]\n",
    "    \n",
    "    if len(acts_valid) == 0:\n",
    "        print(f\"{model_name}: NÃO HÁ PONTOS REAIS PARA AVALIAÇÃO!\")\n",
    "        return np.nan, np.nan, np.nan, model_name\n",
    "    \n",
    "    errors = preds_valid - acts_valid\n",
    "    mse = np.square(errors).mean()\n",
    "    rmse = np.sqrt(mse)\n",
    "    nrmse = rmse / ((acts_valid.max() - acts_valid.min()) + 1e-12)\n",
    "    mae = np.abs(errors).mean()\n",
    "    \n",
    "    print(f'{model_name} (missing-aware):')\n",
    "    print(f'MAE:  {mae:.4f}')\n",
    "    print(f'RMSE: {rmse:.4f}')\n",
    "    print(f'NRMSE: {nrmse*100:.4f}%\\n')\n",
    "    \n",
    "    return rmse, mae, nrmse, model_name\n",
    "\n",
    "# ===================================================================================\n",
    "# FUNÇÕES DE VISUALIZAÇÃO E SALVAMENTO\n",
    "# ===================================================================================\n",
    "\n",
    "def save_model(model, directory, substring_desejada, modelo):\n",
    "    \"\"\"Salva modelo treinado\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    file_path = os.path.join(directory, f'{substring_desejada}_{modelo}_final_model.keras')\n",
    "    model.save(file_path)\n",
    "    print(f\"Modelo salvo como '{file_path}'\")\n",
    "\n",
    "def plot_loss(history, title):\n",
    "    \"\"\"Plota curvas de loss\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(history['loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "def plot_predictions_with_confidence(y_true, y_pred, confidence_scores, mask, title):\n",
    "    \"\"\"Plota predições com scores de confiança\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Predições vs Valores Reais\n",
    "    ax1.plot(y_true, label='Valores Reais', alpha=0.8)\n",
    "    ax1.plot(y_pred, label='Predições', alpha=0.8)\n",
    "    \n",
    "    # Destacar pontos sem dados reais\n",
    "    missing_indices = np.where(~mask)[0]\n",
    "    if len(missing_indices) > 0:\n",
    "        ax1.scatter(missing_indices, np.array(y_pred)[missing_indices], \n",
    "                   color='red', alpha=0.6, s=20, label='Predições em Gaps')\n",
    "    \n",
    "    ax1.set_title(f'{title} - Predições vs Realidade')\n",
    "    ax1.set_xlabel('Tempo')\n",
    "    ax1.set_ylabel('Throughput (Mbps)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Scores de Confiança\n",
    "    ax2.plot(confidence_scores, color='orange', alpha=0.8)\n",
    "    ax2.set_title('Scores de Confiança ao Longo do Tempo')\n",
    "    ax2.set_xlabel('Tempo')\n",
    "    ax2.set_ylabel('Confiança')\n",
    "    ax2.set_ylim(0, 1.1)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ===================================================================================\n",
    "# SCRIPT PRINCIPAL\n",
    "# ===================================================================================\n",
    "\n",
    "# Configurações\n",
    "LOOK_BACK = 3\n",
    "WINDOW_SIZE = 28\n",
    "DECAY_FACTOR = 0.95  # Fator de decaimento para dados imputados\n",
    "CONFIDENCE_THRESHOLD = 0.7\n",
    "\n",
    "# Caminhos\n",
    "TRAINING_OUTPUT = 'training_output.txt'\n",
    "THROUGHPUT_DATASETS = os.path.join('..', '..', 'datasets', 'test-recursive-lstm-test')\n",
    "MODEL_DIR = os.path.join('..', '..', 'modelo_salvo')\n",
    "METRICS_FILE = os.path.join('..', '..', 'results', 'bi-lstm', 'evaluation_decay.json')\n",
    "\n",
    "def main():\n",
    "    \"\"\"Função principal do script\"\"\"\n",
    "    \n",
    "    # Redirecionar saída\n",
    "    orig_stdout = sys.stdout\n",
    "    \n",
    "    with open(TRAINING_OUTPUT, 'w', encoding='utf-8') as f:\n",
    "        sys.stdout = f\n",
    "        \n",
    "        evaluation = {}\n",
    "        diretorio_raiz = THROUGHPUT_DATASETS\n",
    "        \n",
    "        print(\"=== INICIANDO TREINAMENTO COM TRAINING DECAY ===\")\n",
    "        print(f\"Decay Factor: {DECAY_FACTOR}\")\n",
    "        print(f\"Window Size: {WINDOW_SIZE}\")\n",
    "        print(f\"Look Back: {LOOK_BACK}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for pasta_raiz, subpastas, arquivos in os.walk(diretorio_raiz):\n",
    "            for arquivo in arquivos:\n",
    "                if arquivo.endswith('.csv'):\n",
    "                    caminho_arquivo = os.path.join(pasta_raiz, arquivo)\n",
    "                    \n",
    "                    try:\n",
    "                        # Extrai identificador do arquivo\n",
    "                        partes = caminho_arquivo.split(os.sep)\n",
    "                        if len(partes) >= 6:\n",
    "                            substring_desejada = partes[4] + ' - ' + partes[5]\n",
    "                        else:\n",
    "                            substring_desejada = arquivo.replace('.csv', '')\n",
    "                        \n",
    "                        print(f\"\\n{'='*60}\")\n",
    "                        print(f\"PROCESSANDO: {substring_desejada}\")\n",
    "                        print(f\"{'='*60}\")\n",
    "                        \n",
    "                        # Carregamento e pré-processamento\n",
    "                        df = pd.read_csv(caminho_arquivo, index_col='Timestamp')\n",
    "                        \n",
    "                        if '0' in df.columns:\n",
    "                            df.drop('0', axis=1, inplace=True)\n",
    "                        \n",
    "                        # Converter para megabits\n",
    "                        bits_para_megabits(df, 'Throughput')\n",
    "                        \n",
    "                        # Criar máscara ANTES de qualquer manipulação\n",
    "                        mask_total = ~(df['Throughput'].isna() | (df['Throughput'] == -1))\n",
    "                        \n",
    "                        # Split treino-teste\n",
    "                        train_size = int(len(df.index) * 0.8)\n",
    "                        train_data = df.iloc[:train_size].copy()\n",
    "                        test_data = df.iloc[train_size:].copy()\n",
    "                        \n",
    "                        print(f\"Tamanho do treino: {len(train_data)}\")\n",
    "                        print(f\"Tamanho do teste: {len(test_data)}\")\n",
    "                        print(f\"Pontos faltantes no teste: {(~mask_total.iloc[train_size:]).sum()}\")\n",
    "                        \n",
    "                        # Interpolação apenas no treino\n",
    "                        train_data = linear_interpolation(train_data)\n",
    "                        \n",
    "                        # Normalização\n",
    "                        scaler = MinMaxScaler().fit(train_data[['Throughput']])\n",
    "                        train_scaled = scaler.transform(train_data[['Throughput']])\n",
    "                        test_scaled = scaler.transform(test_data[['Throughput']])\n",
    "                        \n",
    "                        mask_test = mask_total.iloc[train_size:].values\n",
    "                        \n",
    "                        # Preparação dos dados para LSTM\n",
    "                        X_train, y_train = create_dataset(train_scaled, LOOK_BACK)\n",
    "                        print(f\"Shape X_train: {X_train.shape}\")\n",
    "                        print(f\"Shape y_train: {y_train.shape}\")\n",
    "                        \n",
    "                        # Grid Search\n",
    "                        print(\"\\n--- INICIANDO GRID SEARCH ---\")\n",
    "                        best_params = grid_search_cv(\n",
    "                            create_lstm, 64, X_train, [1e-3, 5e-4],\n",
    "                            y_train, [100, 300, 500], [32, 64, 128], [10, 15], 'LSTM'\n",
    "                        )\n",
    "                        \n",
    "                        # Treinamento do modelo final\n",
    "                        print(\"\\n--- TREINAMENTO DO MODELO FINAL ---\")\n",
    "                        model = create_lstm(64, X_train, best_params['learning_rate'])\n",
    "                        \n",
    "                        early_stop = EarlyStopping(\n",
    "                            monitor='val_loss',\n",
    "                            patience=best_params['patience'],\n",
    "                            restore_best_weights=True,\n",
    "                            min_delta=1e-6\n",
    "                        )\n",
    "                        \n",
    "                        history = model.fit(\n",
    "                            X_train, y_train,\n",
    "                            epochs=best_params['epochs'],\n",
    "                            batch_size=best_params['batch_size'],\n",
    "                            validation_split=0.2,\n",
    "                            callbacks=[early_stop],\n",
    "                            verbose=1\n",
    "                        )\n",
    "                        \n",
    "                        # Predição com Training Decay\n",
    "                        print(\"\\n--- PREDIÇÃO COM TRAINING DECAY ---\")\n",
    "                        predictions, confidence_scores = walk_forward_validation_with_decay(\n",
    "                            model, scaler, train_scaled, test_scaled,\n",
    "                            mask_test, LOOK_BACK, WINDOW_SIZE, \n",
    "                            decay_factor=DECAY_FACTOR,\n",
    "                            confidence_threshold=CONFIDENCE_THRESHOLD\n",
    "                        )\n",
    "                        \n",
    "                        # Avaliação\n",
    "                        print(\"\\n--- AVALIAÇÃO ---\")\n",
    "                        min_len = min(len(predictions), len(test_scaled) - LOOK_BACK)\n",
    "                        y_test_valid = test_scaled[LOOK_BACK:LOOK_BACK + min_len]\n",
    "                        mask_eval = mask_test[LOOK_BACK:LOOK_BACK + min_len]\n",
    "                        \n",
    "                        y_test_orig = scaler.inverse_transform(y_test_valid)\n",
    "                        predictions_arr = np.array(predictions[:min_len]).reshape(-1, 1)\n",
    "                        \n",
    "                        lstm_eval = validate_missing_data_prediction(\n",
    "                            predictions_arr, y_test_orig, mask_eval, 'LSTM_DECAY'\n",
    "                        )\n",
    "                        \n",
    "                        # Armazenar resultados\n",
    "                        evaluation[f\"{substring_desejada}, LSTM_DECAY\"] = {\n",
    "                            'rmse': float(lstm_eval[0]),\n",
    "                            'mae': float(lstm_eval[1]),\n",
    "                            'nrmse': float(lstm_eval[2]),\n",
    "                            'mean_confidence': float(np.mean(confidence_scores)),\n",
    "                            'min_confidence': float(np.min(confidence_scores)),\n",
    "                            'decay_factor': DECAY_FACTOR\n",
    "                        }\n",
    "                        \n",
    "                        print(f\"\\nRESULTADOS FINAIS PARA {substring_desejada}:\")\n",
    "                        print(f\"RMSE: {lstm_eval[0]:.4f}\")\n",
    "                        print(f\"MAE: {lstm_eval[1]:.4f}\")\n",
    "                        print(f\"NRMSE: {lstm_eval[2]*100:.2f}%\")\n",
    "                        print(f\"Confiança Média: {np.mean(confidence_scores):.3f}\")\n",
    "                        print(f\"Confiança Mínima: {np.min(confidence_scores):.3f}\")\n",
    "                        \n",
    "                        # Salvar modelo\n",
    "                        save_model(model, MODEL_DIR, substring_desejada, 'LSTM_DECAY')\n",
    "                        \n",
    "                        # Restaurar stdout temporariamente para plots\n",
    "                        sys.stdout = orig_stdout\n",
    "                        \n",
    "                        # Plots\n",
    "                        plot_loss(history.history, f'LSTM Training Loss - {substring_desejada}')\n",
    "                        plot_predictions_with_confidence(\n",
    "                            y_test_orig.flatten(), \n",
    "                            predictions_arr.flatten(), \n",
    "                            confidence_scores[:min_len], \n",
    "                            mask_eval,\n",
    "                            f'LSTM com Training Decay - {substring_desejada}'\n",
    "                        )\n",
    "                        \n",
    "                        # Voltar para arquivo\n",
    "                        sys.stdout = f\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"ERRO ao processar {arquivo}: {str(e)}\")\n",
    "                        import traceback\n",
    "                        traceback.print_exc()\n",
    "        \n",
    "        # Restaurar stdout\n",
    "        sys.stdout = orig_stdout\n",
    "    \n",
    "    # Salvar métricas\n",
    "    os.makedirs(os.path.dirname(METRICS_FILE), exist_ok=True)\n",
    "    with open(METRICS_FILE, 'w') as f:\n",
    "        json.dump(evaluation, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nTreinamento concluído! Resultados salvos em:\")\n",
    "    print(f\"- Log: {TRAINING_OUTPUT}\")\n",
    "    print(f\"- Métricas: {METRICS_FILE}\")\n",
    "    print(f\"- Modelos: {MODEL_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
